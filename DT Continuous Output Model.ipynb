{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, classification_report, confusion_matrix\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.feature_selection import RFECV, mutual_info_classif\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, AdaBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load smote_train_data for training data and transformed_data for evaluation\n",
    "smote_train_data_folder_path = '/Users/joanwong/Desktop/cs3244/finalised_datasets/smote_train_data'\n",
    "transformed_data_folder_path = '/Users/joanwong/Desktop/cs3244/finalised_datasets/transformed_data'\n",
    "\n",
    "smote_files = ['smote_fold_1.csv', 'smote_fold_2.csv', 'smote_fold_3.csv', 'smote_fold_4.csv', 'smote_fold_5.csv']\n",
    "non_smote_files = ['fold_1.csv', 'fold_2.csv','fold_3.csv', 'fold_4.csv', 'fold_5.csv']\n",
    "\n",
    "\n",
    "testing_data = pd.read_csv('/Users/joanwong/Desktop/cs3244/finalised_datasets/transformed_data/test_set.csv')\n",
    "testing_data = testing_data.drop(columns = ['client_id', 'creation_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom cross-validation using RFECV for regression\n",
    "feature_counts_rfecv = []\n",
    "selected_features_rfecv_all_folds = []\n",
    "feature_rankings_rfecv = []\n",
    "\n",
    "for i in range(len(smote_files)):\n",
    "    # Define training and testing sets\n",
    "    train_smote_files = [file for j, file in enumerate(smote_files) if j != i]  # All folds except the i-th for training\n",
    "    test_non_smote_file = non_smote_files[i]  # The corresponding non-smoted fold for testing\n",
    "\n",
    "    # Load training data\n",
    "    train_dataframes = []\n",
    "    for file in train_smote_files:\n",
    "        file_path = os.path.join(smote_train_data_folder_path, file)\n",
    "        df = pd.read_csv(file_path)\n",
    "        train_dataframes.append(df)\n",
    "\n",
    "    # Concatenate training data\n",
    "    train_data = pd.concat(train_dataframes, ignore_index=True)\n",
    "\n",
    "    # Load testing data\n",
    "    test_file_path = os.path.join(smote_train_data_folder_path, test_non_smote_file)\n",
    "    test_data = pd.read_csv(test_file_path)\n",
    "\n",
    "    # Split features and target\n",
    "    features = train_data.columns.drop('fraud_status')\n",
    "    X_train = train_data[features]\n",
    "    y_train = train_data['fraud_status']\n",
    "    X_test = test_data[features]\n",
    "    y_test = test_data['fraud_status']\n",
    "\n",
    "    # Step 1: Feature Selection using RFECV\n",
    "    print(f\"Fold {i+1}: Performing RFECV for feature selection...\")\n",
    "    model = DecisionTreeRegressor(random_state=42)\n",
    "    cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "    rfecv = RFECV(estimator=model, step=2, cv=cv, scoring='neg_root_mean_squared_error', n_jobs=-1)\n",
    "    rfecv.fit(X_train, y_train)\n",
    "\n",
    "    # Select the optimal features using RFECV\n",
    "    selected_features_rfecv = X_train.columns[rfecv.support_]\n",
    "    print(f\"Number of features selected by RFECV for Fold {i+1}: {len(selected_features_rfecv)}\")\n",
    "    print(\"Selected features using RFECV:\", selected_features_rfecv)\n",
    "    feature_counts_rfecv.append(len(selected_features_rfecv))\n",
    "    selected_features_rfecv_all_folds.append(selected_features_rfecv)\n",
    "    feature_rankings_rfecv.append(pd.Series(rfecv.ranking_, index=X_train.columns))\n",
    "\n",
    "    # Plot the number of features vs cross-validation score\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_, marker='o')\n",
    "    plt.xlabel('Number of Features')\n",
    "    plt.ylabel('Cross-Validation Score (Neg RMSE)')\n",
    "    plt.title(f'RFECV: Optimal Number of Features for Fold {i + 1}')\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregated Feature Importance for RFECV\n",
    "avg_rankings_rfecv = pd.concat(feature_rankings_rfecv, axis=1).mean(axis=1).sort_values()\n",
    "print(\"\\nAggregated Feature Importance (RFECV):\")\n",
    "print(avg_rankings_rfecv)\n",
    "\n",
    "# Determine Optimal Features using Aggregated Feature Importance (RFECV)\n",
    "rmse_scores_rfecv = []\n",
    "threshold = 0.001  # Set a threshold for what counts as \"sufficiently close to 0\"\n",
    "last_rmse_score = None\n",
    "optimal_num_features_rfecv = None\n",
    "\n",
    "# Loop through increasing numbers of features\n",
    "for i in range(1, len(avg_rankings_rfecv) + 1):\n",
    "    selected_features = avg_rankings_rfecv.head(i).index\n",
    "    X_train_selected = X_train[selected_features]\n",
    "\n",
    "    # Perform cross-validation with the selected features\n",
    "    model = DecisionTreeRegressor(random_state=42)\n",
    "    rmse = -cross_val_score(model, X_train_selected, y_train, cv=3, scoring='neg_root_mean_squared_error').mean()\n",
    "    rmse_scores_rfecv.append(rmse)\n",
    "\n",
    "    # Check if the difference is sufficiently small\n",
    "    if last_rmse_score is not None:\n",
    "        difference = abs(rmse - last_rmse_score)\n",
    "        if difference <= threshold:\n",
    "            optimal_num_features_rfecv = i\n",
    "            break\n",
    "\n",
    "    last_rmse_score = rmse\n",
    "\n",
    "# Plot the RMSE scores vs number of features (show the entire range)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, len(rmse_scores_rfecv) + 1), rmse_scores_rfecv, marker='o')\n",
    "plt.xlabel('Number of Features')\n",
    "plt.ylabel('Cross-Validation RMSE')\n",
    "plt.title('RMSE Scores vs Number of Features (RFECV)')\n",
    "plt.grid()\n",
    "\n",
    "# Mark the optimal number of features with a red dotted line\n",
    "if optimal_num_features_rfecv is not None:\n",
    "    plt.axvline(x=optimal_num_features_rfecv, color='r', linestyle='--', label=f'Optimal Features: {optimal_num_features_rfecv}')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Print the optimal number of features and the selected features\n",
    "if optimal_num_features_rfecv is not None:\n",
    "    print(f\"Optimal number of features where change is sufficiently close to 0 (RFECV): {optimal_num_features_rfecv}\")\n",
    "    selected_features_rfecv_final = avg_rankings_rfecv.head(optimal_num_features_rfecv).index\n",
    "    print(f\"Selected features based on optimal number (RFECV): {list(selected_features_rfecv_final)}\")\n",
    "else:\n",
    "    print(\"No plateau detected within the set threshold for RFECV.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom cross-validation using Mutual Information for regression\n",
    "avg_mutual_info = []\n",
    "feature_counts_mi = []\n",
    "selected_features_mi_all_folds = []\n",
    "\n",
    "for i in range(len(smote_files)):\n",
    "    # Define training and testing sets\n",
    "    train_smote_files = [file for j, file in enumerate(smote_files) if j != i]  # All folds except the i-th for training\n",
    "    test_non_smote_file = non_smote_files[i]  # The corresponding non-smoted fold for testing\n",
    "\n",
    "    # Load training data\n",
    "    train_dataframes = []\n",
    "    for file in train_smote_files:\n",
    "        file_path = os.path.join(smote_train_data_folder_path, file)\n",
    "        df = pd.read_csv(file_path)\n",
    "        train_dataframes.append(df)\n",
    "\n",
    "    # Concatenate training data\n",
    "    train_data = pd.concat(train_dataframes, ignore_index=True)\n",
    "\n",
    "    # Load testing data\n",
    "    test_file_path = os.path.join(smote_train_data_folder_path, test_non_smote_file)\n",
    "    test_data = pd.read_csv(test_file_path)\n",
    "\n",
    "    # Split features and target\n",
    "    features = train_data.columns.drop('fraud_status')\n",
    "    X_train = train_data[features]\n",
    "    y_train = train_data['fraud_status']\n",
    "    X_test = test_data[features]\n",
    "    y_test = test_data['fraud_status']\n",
    "\n",
    "    # Step 1: Feature Selection using Mutual Information\n",
    "    print(f\"Fold {i+1}: Performing Mutual Information for feature selection...\")\n",
    "    mutual_info = mutual_info_regression(X_train, y_train)\n",
    "    mutual_info_series = pd.Series(mutual_info, index=X_train.columns).sort_values(ascending=False)\n",
    "    avg_mutual_info.append(mutual_info_series)\n",
    "    scores = []\n",
    "\n",
    "    # Determine optimal number of features using cross-validation\n",
    "    best_score = -np.inf\n",
    "    optimal_k = 0\n",
    "    for k in range(1, len(mutual_info_series) + 1):\n",
    "        selected_features = mutual_info_series.head(k).index\n",
    "        X_train_selected = X_train[selected_features]\n",
    "        score = cross_val_score(DecisionTreeRegressor(random_state=42), X_train_selected, y_train, cv=3, scoring='neg_root_mean_squared_error').mean()\n",
    "        scores.append(score)\n",
    "\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            optimal_k = k\n",
    "\n",
    "    print(f\"Optimal number of features based on cross-validation for Fold {i + 1}: {optimal_k}\")\n",
    "    selected_features_mi_optimal = mutual_info_series.head(optimal_k).index\n",
    "    print(\"Selected features using Mutual Information:\", selected_features_mi_optimal)\n",
    "    feature_counts_mi.append(len(selected_features_mi_optimal))\n",
    "    selected_features_mi_all_folds.append(selected_features_mi_optimal)\n",
    "\n",
    "    # Plot the cross-validation scores vs. number of features\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(range(1, len(mutual_info_series) + 1), scores, marker='o')\n",
    "    plt.xlabel('Number of Features')\n",
    "    plt.ylabel('Cross-Validation Score (Neg RMSE)')\n",
    "    plt.title(f'Mutual Information: Optimal Number of Features for Fold {i + 1}')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregated Feature Importance for Mutual Information\n",
    "avg_mutual_info_df = pd.concat(avg_mutual_info, axis=1).mean(axis=1).sort_values(ascending=False)\n",
    "print(\"\\nAggregated Feature Importance (Mutual Information):\")\n",
    "print(avg_mutual_info_df)\n",
    "\n",
    "# Determine Optimal Features using Aggregated Feature Importance (Mutual Information)\n",
    "rmse_scores = []\n",
    "threshold = 0.001  # Set a threshold for what counts as \"sufficiently close to 0\"\n",
    "last_rmse_score = None\n",
    "optimal_num_features = None\n",
    "\n",
    "# Loop through increasing numbers of features\n",
    "for i in range(1, len(avg_mutual_info_df) + 1):\n",
    "    selected_features = avg_mutual_info_df.head(i).index\n",
    "    X_train_selected = X_train[selected_features]\n",
    "\n",
    "    # Perform cross-validation with the selected features\n",
    "    model = DecisionTreeRegressor(random_state=42)\n",
    "    rmse = -cross_val_score(model, X_train_selected, y_train, cv=3, scoring='neg_root_mean_squared_error').mean()\n",
    "    rmse_scores.append(rmse)\n",
    "\n",
    "    # Check if the difference is sufficiently small\n",
    "    if last_rmse_score is not None:\n",
    "        difference = abs(rmse - last_rmse_score)\n",
    "        if difference <= threshold:\n",
    "            optimal_num_features = i\n",
    "            break\n",
    "\n",
    "    last_rmse_score = rmse\n",
    "\n",
    "# Plot the RMSE scores vs number of features (show the entire range)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, len(rmse_scores) + 1), rmse_scores, marker='o')\n",
    "plt.xlabel('Number of Features')\n",
    "plt.ylabel('Cross-Validation RMSE')\n",
    "plt.title('RMSE Scores vs Number of Features')\n",
    "plt.grid()\n",
    "\n",
    "# Mark the optimal number of features with a red dotted line\n",
    "if optimal_num_features is not None:\n",
    "    plt.axvline(x=optimal_num_features, color='r', linestyle='--', label=f'Optimal Features: {optimal_num_features}')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Print the optimal number of features and the selected features\n",
    "if optimal_num_features is not None:\n",
    "    print(f\"Optimal number of features where change is sufficiently close to 0: {optimal_num_features}\")\n",
    "    selected_features_final = avg_mutual_info_df.head(optimal_num_features).index\n",
    "    print(f\"Selected features based on optimal number: {list(selected_features_final)}\")\n",
    "else:\n",
    "    print(\"No plateau detected within the set threshold.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the complete training and testing datasets\n",
    "train_data = pd.concat([pd.read_csv(os.path.join(smote_train_data_folder_path, file)) for file in smote_files], ignore_index=True)\n",
    "test_data = pd.read_csv(os.path.join(transformed_data_folder_path, 'test_set.csv'))\n",
    "\n",
    "# Split features and target for training and testing\n",
    "features = train_data.columns.drop('fraud_status')\n",
    "X_train_full = train_data[features]\n",
    "y_train_full = train_data['fraud_status']\n",
    "X_test_full = test_data[features]\n",
    "y_test_full = test_data['fraud_status']\n",
    "\n",
    "\n",
    "# Create a sample with a stratified split to maintain class balance\n",
    "sample_fraction = 0.1\n",
    "X_sample, X_sample_test, y_sample, y_sample_test = train_test_split(X_train_full, y_train_full, test_size=(1 - sample_fraction), stratify=y_train_full, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7.1: Use Optuna to choose the best feature selection method (RFECV or Mutual Information)\n",
    "def feature_selection_objective(trial):\n",
    "    feature_method = trial.suggest_categorical('feature_method', ['RFECV', 'Mutual Information'])\n",
    "    \n",
    "    # Use the full dataset for training and testing\n",
    "    if feature_method == 'RFECV':\n",
    "        features = selected_features_rfecv_all_folds[-1]\n",
    "    else:\n",
    "        features = selected_features_mi_all_folds[-1]\n",
    "\n",
    "    X_train_selected = X_train[features]\n",
    "    y_train_selected = y_train\n",
    "\n",
    "    # Train and evaluate model (DecisionTreeRegressor for consistency)\n",
    "    model = DecisionTreeRegressor(random_state=42)\n",
    "    rmse = -cross_val_score(model, X_train_selected, y_train_selected, cv=3, scoring='neg_root_mean_squared_error').mean()\n",
    "\n",
    "    return rmse\n",
    "\n",
    "# Run Optuna to find the best feature selection method\n",
    "feature_study = optuna.create_study(direction='minimize')\n",
    "feature_study.optimize(feature_selection_objective, n_trials=10)\n",
    "\n",
    "best_feature_method = feature_study.best_trial.params['feature_method']\n",
    "print(\"Best feature selection method:\", best_feature_method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7.2: Use Optuna to choose the best ensemble model (Bagging/Random Forest/XGBoost/AdaBoost/LightGBM)\n",
    "def model_selection_objective(trial):\n",
    "    # Use the best feature selection method obtained from Step 7.1\n",
    "    features = selected_features_rfecv_all_folds[-1] if best_feature_method == 'RFECV' else selected_features_mi_all_folds[-1]\n",
    "\n",
    "    X_train_selected = X_train[features]\n",
    "    y_train_selected = y_train\n",
    "\n",
    "    # Choose the ensemble model type and hyperparameters\n",
    "    model_name = trial.suggest_categorical('model_name', ['Bagging', 'RandomForest', 'AdaBoost', 'XGBoost', 'LightGBM'])\n",
    "    \n",
    "    if model_name == 'Bagging':\n",
    "        n_estimators = trial.suggest_int('n_estimators', 10, 100, step=10)\n",
    "        max_samples = trial.suggest_float('max_samples', 0.5, 1.0, step=0.1)\n",
    "        model = BaggingRegressor(estimator=DecisionTreeRegressor(), n_estimators=n_estimators, max_samples=max_samples, random_state=42)\n",
    "    elif model_name == 'RandomForest':\n",
    "        n_estimators = trial.suggest_int('n_estimators', 50, 150, step=50)\n",
    "        max_depth = trial.suggest_int('max_depth', 10, 30, step=10)\n",
    "        model = RandomForestRegressor(n_estimators=n_estimators, max_depth=max_depth, random_state=42)\n",
    "    elif model_name == 'AdaBoost':\n",
    "        n_estimators = trial.suggest_int('n_estimators', 50, 150, step=50)\n",
    "        learning_rate = trial.suggest_float('learning_rate', 0.01, 1.0, log=True)\n",
    "        model = AdaBoostRegressor(estimator=DecisionTreeRegressor(), n_estimators=n_estimators, learning_rate=learning_rate, random_state=42)\n",
    "    elif model_name == 'XGBoost':\n",
    "        n_estimators = trial.suggest_int('n_estimators', 50, 150, step=50)\n",
    "        learning_rate = trial.suggest_float('learning_rate', 0.01, 0.2, log=True)\n",
    "        max_depth = trial.suggest_int('max_depth', 3, 9, step=3)\n",
    "        model = XGBRegressor(n_estimators=n_estimators, learning_rate=learning_rate, max_depth=max_depth, random_state=42)\n",
    "    elif model_name == 'LightGBM':\n",
    "        n_estimators = trial.suggest_int('n_estimators', 50, 150, step=50)\n",
    "        learning_rate = trial.suggest_float('learning_rate', 0.01, 0.2, log=True)\n",
    "        num_leaves = trial.suggest_int('num_leaves', 31, 100, step=10)\n",
    "        model = LGBMRegressor(n_estimators=n_estimators, learning_rate=learning_rate, num_leaves=num_leaves, random_state=42)\n",
    "\n",
    "    rmse = -cross_val_score(model, X_train_selected, y_train_selected, cv=3, scoring='neg_root_mean_squared_error').mean()\n",
    "\n",
    "    return rmse\n",
    "\n",
    "# Run Optuna to find the best ensemble model\n",
    "model_study = optuna.create_study(direction='minimize')\n",
    "model_study.optimize(model_selection_objective, n_trials=20)\n",
    "\n",
    "print(\"Best model selection trial:\")\n",
    "print(model_study.best_trial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the best parameters to initialize and train the model\n",
    "model_name = best_params['model_name']\n",
    "\n",
    "if model_name == 'Bagging':\n",
    "    best_model = BaggingRegressor(\n",
    "        estimator=DecisionTreeRegressor(),\n",
    "        n_estimators=best_params['n_estimators'],\n",
    "        max_samples=best_params['max_samples'],\n",
    "        random_state=42\n",
    "    )\n",
    "elif model_name == 'RandomForest':\n",
    "    best_model = RandomForestRegressor(\n",
    "        n_estimators=best_params['n_estimators'],\n",
    "        max_depth=best_params['max_depth'],\n",
    "        random_state=42\n",
    "    )\n",
    "elif model_name == 'AdaBoost':\n",
    "    best_model = AdaBoostRegressor(\n",
    "        estimator=DecisionTreeRegressor(),\n",
    "        n_estimators=best_params['n_estimators'],\n",
    "        learning_rate=best_params['learning_rate'],\n",
    "        random_state=42\n",
    "    )\n",
    "elif model_name == 'XGBoost':\n",
    "    best_model = XGBRegressor(\n",
    "        n_estimators=best_params['n_estimators'],\n",
    "        learning_rate=best_params['learning_rate'],\n",
    "        max_depth=best_params['max_depth'],\n",
    "        random_state=42,\n",
    "        eval_metric='rmse'\n",
    "    )\n",
    "elif model_name == 'LightGBM':\n",
    "    best_model = LGBMRegressor(\n",
    "        n_estimators=best_params['n_estimators'],\n",
    "        learning_rate=best_params['learning_rate'],\n",
    "        num_leaves=best_params['num_leaves'],\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "# Train the model\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = best_model.predict(X_test_final_selected)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "rmse = np.sqrt(mean_squared_error(y_test_final, y_pred))\n",
    "mae = mean_absolute_error(y_test_final, y_pred)\n",
    "r2 = r2_score(y_test_final, y_pred)\n",
    "\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
    "print(f\"R-squared (R2): {r2:.4f}\")\n",
    "\n",
    "# If evaluating as a classification report for threshold-based results, use classification metrics\n",
    "# You can use a threshold to transform regression outputs to a binary classification (e.g., y_pred_class = y_pred > 0.5)\n",
    "threshold = 0.5\n",
    "y_pred_class = (y_pred > threshold).astype(int)\n",
    "\n",
    "print(\"\\nClassification Report (using threshold for binary output):\")\n",
    "print(classification_report(y_test_final, y_pred_class))\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test_final, y_pred_class))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
