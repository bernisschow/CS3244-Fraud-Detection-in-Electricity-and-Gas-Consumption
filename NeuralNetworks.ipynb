{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS3244 Project Neural Networks\n",
    "\n",
    "This file will contain the code for the neural network model that is being used for our project. In this code, we will be approaching it using a two types of models:\n",
    "\n",
    "1. MLP\n",
    "2. RNN (LSTM)\n",
    "\n",
    "We will follow this breakdown of sequencing in how we approach the problem using neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing datasets and packages\n",
    "\n",
    "We will begin with importing all relevant datasets and packages needed in order to create our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import torch.nn.init as init\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from torch.utils.data import WeightedRandomSampler\n",
    "import optuna\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MLP datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold1 = pd.read_csv('finalised_datasets/smote_train_data/fold_1.csv')\n",
    "fold2 = pd.read_csv('finalised_datasets/smote_train_data/fold_2.csv')\n",
    "fold3 = pd.read_csv('finalised_datasets/smote_train_data/fold_3.csv')\n",
    "fold4 = pd.read_csv('finalised_datasets/smote_train_data/fold_4.csv')\n",
    "fold5 = pd.read_csv('finalised_datasets/smote_train_data/fold_5.csv')\n",
    "\n",
    "smote_fold_1 = pd.read_csv('finalised_datasets/smote_train_data/smote_fold_1.csv')\n",
    "smote_fold_2 = pd.read_csv('finalised_datasets/smote_train_data/smote_fold_2.csv')\n",
    "smote_fold_3 = pd.read_csv('finalised_datasets/smote_train_data/smote_fold_3.csv')\n",
    "smote_fold_4 = pd.read_csv('finalised_datasets/smote_train_data/smote_fold_4.csv')\n",
    "smote_fold_5 = pd.read_csv('finalised_datasets/smote_train_data/smote_fold_5.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('finalised_datasets/transformed_data/test_set.csv')\n",
    "\n",
    "X_test = test_df.drop(columns=['fraud_status'])\n",
    "y_test = test_df['fraud_status']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RNN datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full data\n",
    "invoice = pd.read_csv('finalised_datasets/cleaned_data/cleaned_invoice_train.csv')\n",
    "client = pd.read_csv('finalised_datasets/cleaned_data/cleaned_client_train.csv')\n",
    "\n",
    "# Competition output data\n",
    "invoice_output = pd.read_csv('finalised_datasets/cleaned_data/cleaned_invoice_test.csv')\n",
    "client_output = pd.read_csv('finalised_datasets/cleaned_data/cleaned_client_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP model\n",
    "\n",
    "The MLP model is a simple feed-forward neural network model. It will use the universally used aggregated dataset that we have used for all other models. We have decided to use an MLP model as it is the best neural network model when it comes to working with our aggregated dataset. This is because RNNs work with sequential data, which the aggregated dataset does not have, and CNN works best for matrix data (images), which again we do not have.\n",
    "\n",
    "We will proceed with this flow:\n",
    "1. Feature engineering for MLP\n",
    "2. Model MLP\n",
    "3. Evaluate MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering for MLP\n",
    "\n",
    "We will begin with feature engineering that is specific to the MLP model. This is because neural networks require specific altercations to the data for it to function effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_smoted_1 = smote_fold_1.drop(columns='fraud_status')\n",
    "y_smoted_1 = smote_fold_1['fraud_status']\n",
    "X_smoted_2 = smote_fold_2.drop(columns='fraud_status')\n",
    "y_smoted_2 = smote_fold_2['fraud_status']\n",
    "X_smoted_3 = smote_fold_3.drop(columns='fraud_status')\n",
    "y_smoted_3 = smote_fold_3['fraud_status']\n",
    "X_smoted_4 = smote_fold_4.drop(columns='fraud_status')\n",
    "y_smoted_4 = smote_fold_4['fraud_status']\n",
    "X_smoted_5 = smote_fold_5.drop(columns='fraud_status')\n",
    "y_smoted_5 = smote_fold_5['fraud_status']\n",
    "smoted_folds = [(X_smoted_1, y_smoted_1), (X_smoted_2, y_smoted_2), (X_smoted_3, y_smoted_3), (X_smoted_4, y_smoted_4), (X_smoted_5, y_smoted_5)]\n",
    "\n",
    "X_fold_1 = fold1.drop(columns='fraud_status')\n",
    "y_fold_1 = fold1['fraud_status']\n",
    "X_fold_2 = fold2.drop(columns='fraud_status')\n",
    "y_fold_2 = fold2['fraud_status']\n",
    "X_fold_3 = fold3.drop(columns='fraud_status')\n",
    "y_fold_3 = fold3['fraud_status']\n",
    "X_fold_4 = fold4.drop(columns='fraud_status')\n",
    "y_fold_4 = fold4['fraud_status']\n",
    "X_fold_5 = fold5.drop(columns='fraud_status')\n",
    "y_fold_5 = fold5['fraud_status']\n",
    "folds = [(X_fold_1, y_fold_1), (X_fold_2, y_fold_2), (X_fold_3, y_fold_3), (X_fold_4, y_fold_4), (X_fold_5, y_fold_5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler on the entire training data (folds[0][0])\n",
    "scaler.fit(folds[0][0])\n",
    "\n",
    "# Modify the sample_fold function to scale the input data\n",
    "def sample_fold(X, y, sample_fraction=0.1):\n",
    "    sample_size = int(len(X) * sample_fraction)\n",
    "    sample_indices = np.random.choice(len(X), sample_size, replace=False)\n",
    "    X_sample = scaler.transform(X.iloc[sample_indices])  # Scale X_sample\n",
    "    return pd.DataFrame(X_sample, columns=X.columns), y.iloc[sample_indices]\n",
    "\n",
    "# Take 50% samples of each original and SMOTEd fold (and apply scaling)\n",
    "sampled_folds = [sample_fold(X, y) for X, y in folds]\n",
    "sampled_smoted_folds = [sample_fold(X, y) for X, y in smoted_folds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-30 21:26:56,280] A new study created in memory with name: no-name-a1ce5bf6-4809-4fc8-8718-23a96361d56f\n",
      "Best trial: 6. Best value: 0.910102:   3%|▎         | 1/30 [04:25<2:08:10, 265.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-11-30 21:31:21,521] Trial 6 finished with value: 0.9101022441399745 and parameters: {'hidden_layer_sizes': '50', 'activation': 'tanh', 'solver': 'sgd', 'learning_rate_init': 0.0007521678472856041, 'alpha': 1.1303034751119662e-05}. Best is trial 6 with value: 0.9101022441399745.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 0.914405:   7%|▋         | 2/30 [05:55<1:15:52, 162.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-11-30 21:32:52,328] Trial 0 finished with value: 0.9144051838460239 and parameters: {'hidden_layer_sizes': '50', 'activation': 'relu', 'solver': 'adam', 'learning_rate_init': 0.00013534326001626883, 'alpha': 1.4424078811677292e-05}. Best is trial 0 with value: 0.9144051838460239.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 0.914405:  10%|█         | 3/30 [06:12<43:09, 95.92s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-11-30 21:33:08,871] Trial 5 finished with value: 0.9116317826174039 and parameters: {'hidden_layer_sizes': '100', 'activation': 'logistic', 'solver': 'sgd', 'learning_rate_init': 0.004548057421059331, 'alpha': 0.006102929324898914}. Best is trial 0 with value: 0.9144051838460239.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 0.914405:  13%|█▎        | 4/30 [08:45<51:15, 118.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-11-30 21:35:41,438] Trial 8 finished with value: 0.9143310211285604 and parameters: {'hidden_layer_sizes': '50', 'activation': 'relu', 'solver': 'adam', 'learning_rate_init': 0.0010828760908503223, 'alpha': 0.00872890523794599}. Best is trial 0 with value: 0.9144051838460239.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 4. Best value: 0.916741:  17%|█▋        | 5/30 [09:27<37:51, 90.88s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-11-30 21:36:23,731] Trial 4 finished with value: 0.9167405271676607 and parameters: {'hidden_layer_sizes': '100', 'activation': 'relu', 'solver': 'adam', 'learning_rate_init': 0.00019953790528729157, 'alpha': 2.183626237778898e-05}. Best is trial 4 with value: 0.9167405271676607.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 4. Best value: 0.916741:  20%|██        | 6/30 [12:20<47:29, 118.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-11-30 21:39:16,535] Trial 3 finished with value: 0.3956295130710629 and parameters: {'hidden_layer_sizes': '100_100_50', 'activation': 'logistic', 'solver': 'sgd', 'learning_rate_init': 0.0010135021474661566, 'alpha': 0.0015730466126038112}. Best is trial 4 with value: 0.9167405271676607.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 4. Best value: 0.916741:  23%|██▎       | 7/30 [12:57<35:14, 91.95s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-11-30 21:39:53,344] Trial 2 finished with value: 0.9143328859462783 and parameters: {'hidden_layer_sizes': '100_100_50', 'activation': 'relu', 'solver': 'adam', 'learning_rate_init': 0.0010459810121298948, 'alpha': 0.00015906061887979183}. Best is trial 4 with value: 0.9167405271676607.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 4. Best value: 0.916741:  27%|██▋       | 8/30 [13:04<23:53, 65.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-11-30 21:40:01,083] Trial 10 finished with value: 0.9138207048847538 and parameters: {'hidden_layer_sizes': '100', 'activation': 'tanh', 'solver': 'sgd', 'learning_rate_init': 0.0013372666507146043, 'alpha': 1.1735428168217842e-05}. Best is trial 4 with value: 0.9167405271676607.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 4. Best value: 0.916741:  30%|███       | 9/30 [13:35<19:05, 54.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-11-30 21:40:32,341] Trial 7 finished with value: 0.91323142750917 and parameters: {'hidden_layer_sizes': '100_100_50', 'activation': 'tanh', 'solver': 'adam', 'learning_rate_init': 0.0018099323352968903, 'alpha': 0.001289109846324971}. Best is trial 4 with value: 0.9167405271676607.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 4. Best value: 0.916741:  33%|███▎      | 10/30 [15:36<24:56, 74.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-11-30 21:42:32,537] Trial 15 finished with value: 0.9136524866271897 and parameters: {'hidden_layer_sizes': '50', 'activation': 'relu', 'solver': 'sgd', 'learning_rate_init': 0.0044174570310143305, 'alpha': 6.702066059653442e-05}. Best is trial 4 with value: 0.9167405271676607.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 4. Best value: 0.916741:  37%|███▋      | 11/30 [15:52<17:58, 56.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-11-30 21:42:48,395] Trial 1 finished with value: 0.9144402753148938 and parameters: {'hidden_layer_sizes': '100_100_50', 'activation': 'tanh', 'solver': 'sgd', 'learning_rate_init': 0.002282905424323878, 'alpha': 0.002328146984843629}. Best is trial 4 with value: 0.9167405271676607.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 4. Best value: 0.916741:  40%|████      | 12/30 [16:23<14:44, 49.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-11-30 21:43:20,008] Trial 9 finished with value: 0.9152694380295312 and parameters: {'hidden_layer_sizes': '100_100_50', 'activation': 'logistic', 'solver': 'adam', 'learning_rate_init': 0.006207075385521325, 'alpha': 0.0002511238620390322}. Best is trial 4 with value: 0.9167405271676607.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 4. Best value: 0.916741:  43%|████▎     | 13/30 [16:28<10:08, 35.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-11-30 21:43:25,214] Trial 13 finished with value: 0.9147324200370491 and parameters: {'hidden_layer_sizes': '50', 'activation': 'relu', 'solver': 'sgd', 'learning_rate_init': 0.006718982056411098, 'alpha': 1.1914834182407825e-05}. Best is trial 4 with value: 0.9167405271676607.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 4. Best value: 0.916741:  47%|████▋     | 14/30 [17:59<13:56, 52.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-11-30 21:44:55,647] Trial 16 finished with value: 0.9140179202851755 and parameters: {'hidden_layer_sizes': '50', 'activation': 'tanh', 'solver': 'adam', 'learning_rate_init': 0.00024679748403994884, 'alpha': 0.0014699847961316718}. Best is trial 4 with value: 0.9167405271676607.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 4. Best value: 0.916741:  50%|█████     | 15/30 [18:49<12:56, 51.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-11-30 21:45:46,204] Trial 11 finished with value: 0.9148043794333185 and parameters: {'hidden_layer_sizes': '100_50', 'activation': 'logistic', 'solver': 'adam', 'learning_rate_init': 0.0019764835452693403, 'alpha': 0.00010344172237194732}. Best is trial 4 with value: 0.9167405271676607.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 4. Best value: 0.916741:  53%|█████▎    | 16/30 [21:55<21:27, 92.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-11-30 21:48:51,581] Trial 12 finished with value: 0.906951525564266 and parameters: {'hidden_layer_sizes': '50', 'activation': 'logistic', 'solver': 'sgd', 'learning_rate_init': 0.0005248342640090552, 'alpha': 0.0028245576650024803}. Best is trial 4 with value: 0.9167405271676607.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 4. Best value: 0.916741:  57%|█████▋    | 17/30 [24:18<23:16, 107.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-11-30 21:51:14,827] Trial 22 finished with value: 0.9118643790228559 and parameters: {'hidden_layer_sizes': '100', 'activation': 'logistic', 'solver': 'adam', 'learning_rate_init': 0.00041277585567767747, 'alpha': 3.9395394819425524e-05}. Best is trial 4 with value: 0.9167405271676607.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 4. Best value: 0.916741:  60%|██████    | 18/30 [27:28<26:24, 132.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-11-30 21:54:24,353] Trial 23 finished with value: 0.912223971897243 and parameters: {'hidden_layer_sizes': '100', 'activation': 'logistic', 'solver': 'adam', 'learning_rate_init': 0.00039335263700698375, 'alpha': 0.0005591104623720087}. Best is trial 4 with value: 0.9167405271676607.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 4. Best value: 0.916741:  63%|██████▎   | 19/30 [27:28<16:57, 92.49s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-11-30 21:54:24,597] Trial 20 finished with value: 0.9105679436369909 and parameters: {'hidden_layer_sizes': '100_50', 'activation': 'logistic', 'solver': 'adam', 'learning_rate_init': 0.00017845946788075961, 'alpha': 4.957362086726894e-05}. Best is trial 4 with value: 0.9167405271676607.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 4. Best value: 0.916741:  67%|██████▋   | 20/30 [27:30<10:53, 65.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-11-30 21:54:26,549] Trial 19 finished with value: 0.9125333764048543 and parameters: {'hidden_layer_sizes': '100_50', 'activation': 'logistic', 'solver': 'adam', 'learning_rate_init': 0.0002987399058683469, 'alpha': 4.9688747261759696e-05}. Best is trial 4 with value: 0.9167405271676607.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 4. Best value: 0.916741:  70%|███████   | 21/30 [27:30<06:52, 45.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-11-30 21:54:27,059] Trial 14 finished with value: 0.9136756544692576 and parameters: {'hidden_layer_sizes': '100_50', 'activation': 'relu', 'solver': 'sgd', 'learning_rate_init': 0.001049407558034335, 'alpha': 0.0023584796020927005}. Best is trial 4 with value: 0.9167405271676607.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 4. Best value: 0.916741:  73%|███████▎  | 22/30 [29:18<08:34, 64.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-11-30 21:56:14,424] Trial 17 finished with value: 0.9158570830476964 and parameters: {'hidden_layer_sizes': '100_50', 'activation': 'relu', 'solver': 'adam', 'learning_rate_init': 0.00010049897195945209, 'alpha': 5.4553448720973204e-05}. Best is trial 4 with value: 0.9167405271676607.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 4. Best value: 0.916741:  77%|███████▋  | 23/30 [34:04<15:15, 130.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-11-30 22:01:00,409] Trial 18 finished with value: 0.9145666020536962 and parameters: {'hidden_layer_sizes': '100_50', 'activation': 'tanh', 'solver': 'adam', 'learning_rate_init': 0.00017297598075868357, 'alpha': 0.0009067068359522112}. Best is trial 4 with value: 0.9167405271676607.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 4. Best value: 0.916741:  80%|████████  | 24/30 [34:20<09:38, 96.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-11-30 22:01:16,526] Trial 21 finished with value: 0.9116544342754571 and parameters: {'hidden_layer_sizes': '100_50', 'activation': 'logistic', 'solver': 'adam', 'learning_rate_init': 0.0004389428457363222, 'alpha': 5.563402226489949e-05}. Best is trial 4 with value: 0.9167405271676607.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 27. Best value: 0.916877:  83%|████████▎ | 25/30 [37:17<10:02, 120.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-11-30 22:04:13,447] Trial 27 finished with value: 0.9168771104953727 and parameters: {'hidden_layer_sizes': '100_100_50', 'activation': 'relu', 'solver': 'adam', 'learning_rate_init': 0.008035866825788862, 'alpha': 0.0003348201499203202}. Best is trial 27 with value: 0.9168771104953727.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 27. Best value: 0.916877:  87%|████████▋ | 26/30 [37:45<06:12, 93.02s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-11-30 22:04:42,214] Trial 28 finished with value: 0.9164033526776094 and parameters: {'hidden_layer_sizes': '100_50', 'activation': 'logistic', 'solver': 'adam', 'learning_rate_init': 0.0029604809122482753, 'alpha': 0.00020622285996817893}. Best is trial 27 with value: 0.9168771104953727.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 27. Best value: 0.916877:  90%|█████████ | 27/30 [38:01<03:28, 69.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-11-30 22:04:57,318] Trial 26 finished with value: 0.9130335424496941 and parameters: {'hidden_layer_sizes': '100_100_50', 'activation': 'relu', 'solver': 'adam', 'learning_rate_init': 0.0095146183072615, 'alpha': 0.0003140922796200884}. Best is trial 27 with value: 0.9168771104953727.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 27. Best value: 0.916877:  93%|█████████▎| 28/30 [38:01<01:38, 49.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-11-30 22:04:58,176] Trial 24 finished with value: 0.9110551458943679 and parameters: {'hidden_layer_sizes': '100_50', 'activation': 'logistic', 'solver': 'adam', 'learning_rate_init': 0.00012790100678820428, 'alpha': 0.0003675400673571145}. Best is trial 27 with value: 0.9168771104953727.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 27. Best value: 0.916877:  97%|█████████▋| 29/30 [38:14<00:37, 37.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-11-30 22:05:10,361] Trial 29 finished with value: 0.9151501337231309 and parameters: {'hidden_layer_sizes': '100', 'activation': 'relu', 'solver': 'adam', 'learning_rate_init': 0.00010321384226645222, 'alpha': 0.0003597785619496541}. Best is trial 27 with value: 0.9168771104953727.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 27. Best value: 0.916877: 100%|██████████| 30/30 [38:17<00:00, 76.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-11-30 22:05:14,048] Trial 25 finished with value: 0.9163960603495642 and parameters: {'hidden_layer_sizes': '100_50', 'activation': 'relu', 'solver': 'adam', 'learning_rate_init': 0.00010146981801967016, 'alpha': 0.00030740700980334785}. Best is trial 27 with value: 0.9168771104953727.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score, confusion_matrix, classification_report\n",
    "\n",
    "def objective(trial):\n",
    "    hidden_layer_sizes_options = ['100_50', '50', '100', '100_100_50']\n",
    "    hidden_layer_sizes_str = trial.suggest_categorical('hidden_layer_sizes', hidden_layer_sizes_options)\n",
    "    hidden_layer_sizes = tuple(map(int, hidden_layer_sizes_str.split('_')))\n",
    "    activation = trial.suggest_categorical('activation', ['relu', 'tanh', 'logistic'])\n",
    "    solver = trial.suggest_categorical('solver', ['adam', 'sgd'])\n",
    "    learning_rate_init = trial.suggest_float('learning_rate_init', 1e-4, 1e-2, log=True)\n",
    "    alpha = trial.suggest_float('alpha', 1e-5, 1e-2, log=True)  # Added alpha for L2 regularization\n",
    "\n",
    "    # Initialize the MLPClassifier with the current hyperparameters\n",
    "    mlp = MLPClassifier(\n",
    "        hidden_layer_sizes=hidden_layer_sizes,\n",
    "        activation=activation,\n",
    "        solver=solver,\n",
    "        learning_rate_init=learning_rate_init,\n",
    "        max_iter=1000,  # Set a high value for max_iter\n",
    "        alpha=alpha,\n",
    "        early_stopping=True,  # Enable early stopping\n",
    "        validation_fraction=0.1,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # List to store F1 scores for each fold\n",
    "    f1_scores = []\n",
    "\n",
    "    # Perform 5-fold cross-validation\n",
    "    for i in range(5):\n",
    "        # Get the non-SMOTEd validation set (sampled)\n",
    "        X_val, y_val = sampled_folds[i]\n",
    "\n",
    "        # Combine the other 4 SMOTEd folds to create the training set (sampled)\n",
    "        X_train = pd.concat([sampled_smoted_folds[j][0] for j in range(5) if j != i], axis=0)\n",
    "        y_train = pd.concat([sampled_smoted_folds[j][1] for j in range(5) if j != i], axis=0)\n",
    "\n",
    "        # Train the MLP model on the sampled SMOTEd training set\n",
    "        mlp.fit(X_train, y_train)\n",
    "\n",
    "        # Predict on the non-SMOTEd validation set\n",
    "        y_pred = mlp.predict(X_val)\n",
    "\n",
    "        # Calculate F1 score and store it\n",
    "        f1 = f1_score(y_val, y_pred, average='weighted')\n",
    "        f1_scores.append(f1)\n",
    "\n",
    "    # Return the mean F1 score across all folds\n",
    "    return np.mean(f1_scores)\n",
    "\n",
    "# Use Optuna to find the best hyperparameters\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=30, show_progress_bar=True, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'hidden_layer_sizes': '100_100_50', 'activation': 'relu', 'solver': 'adam', 'learning_rate_init': 0.008035866825788862, 'alpha': 0.0003348201499203202}\n",
      "Best cross-validation F1-score: 0.9168771104953727\n"
     ]
    }
   ],
   "source": [
    "# Get the best parameters from Optuna\n",
    "best_trial = study.best_trial\n",
    "best_params = study.best_params\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "print(\"Best cross-validation F1-score:\", study.best_value)\n",
    "\n",
    "# Extract hyperparameters from the best trial found by Optuna\n",
    "hidden_layer_sizes_str = best_params['hidden_layer_sizes']\n",
    "if '_' in hidden_layer_sizes_str:\n",
    "    best_hidden_layer_sizes = tuple(map(int, hidden_layer_sizes_str.split('_')))\n",
    "else:\n",
    "    best_hidden_layer_sizes = [int(hidden_layer_sizes_str)]\n",
    "\n",
    "activation_function = best_params['activation']\n",
    "learning_rate = best_params['learning_rate_init']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training MLP model\n",
    "\n",
    "We will train the neural network on our feature engineered train dataset. Our hyperparameters will be kept fixed (for now), and we will be using the scikit-learn inbuilt function. This is because we were having problems with tensorflow and its compatibility with pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all SMOTEd folds for final training\n",
    "X_train_final = pd.concat([fold[0] for fold in smoted_folds], axis=0)\n",
    "y_train_final = pd.concat([fold[1] for fold in smoted_folds], axis=0)\n",
    "\n",
    "# Ensure that X_train_final is a DataFrame with feature names\n",
    "X_train_final = pd.DataFrame(X_train_final, columns=folds[0][0].columns)\n",
    "\n",
    "# Standardize the final training set\n",
    "scaler_final = StandardScaler()\n",
    "X_train_final_scaled = scaler_final.fit_transform(X_train_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPClassifier(\n",
    "    hidden_layer_sizes=best_hidden_layer_sizes,\n",
    "    activation=activation_function,\n",
    "    solver=best_params['solver'],\n",
    "    learning_rate_init=learning_rate,\n",
    "    max_iter=200,\n",
    "    random_state=42,\n",
    "    verbose=True,\n",
    "    early_stopping=True,  # Enable early stopping for final training\n",
    "    validation_fraction=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.17536473\n",
      "Validation score: 0.935983\n",
      "Iteration 2, loss = 0.15456643\n",
      "Validation score: 0.934176\n",
      "Iteration 3, loss = 0.14759604\n",
      "Validation score: 0.940866\n",
      "Iteration 4, loss = 0.14075227\n",
      "Validation score: 0.942380\n",
      "Iteration 5, loss = 0.13821278\n",
      "Validation score: 0.941062\n",
      "Iteration 6, loss = 0.13355825\n",
      "Validation score: 0.944089\n",
      "Iteration 7, loss = 0.12949766\n",
      "Validation score: 0.944919\n",
      "Iteration 8, loss = 0.12684938\n",
      "Validation score: 0.945163\n",
      "Iteration 9, loss = 0.12363726\n",
      "Validation score: 0.948826\n",
      "Iteration 10, loss = 0.12099153\n",
      "Validation score: 0.948240\n",
      "Iteration 11, loss = 0.11918848\n",
      "Validation score: 0.947556\n",
      "Iteration 12, loss = 0.11489572\n",
      "Validation score: 0.949314\n",
      "Iteration 13, loss = 0.11444192\n",
      "Validation score: 0.949412\n",
      "Iteration 14, loss = 0.11136169\n",
      "Validation score: 0.945505\n",
      "Iteration 15, loss = 0.11001868\n",
      "Validation score: 0.950632\n",
      "Iteration 16, loss = 0.10752824\n",
      "Validation score: 0.950437\n",
      "Iteration 17, loss = 0.10589742\n",
      "Validation score: 0.952146\n",
      "Iteration 18, loss = 0.10528734\n",
      "Validation score: 0.950828\n",
      "Iteration 19, loss = 0.10437665\n",
      "Validation score: 0.951609\n",
      "Iteration 20, loss = 0.10205636\n",
      "Validation score: 0.950925\n",
      "Iteration 21, loss = 0.10032568\n",
      "Validation score: 0.953220\n",
      "Iteration 22, loss = 0.09935798\n",
      "Validation score: 0.953318\n",
      "Iteration 23, loss = 0.09924709\n",
      "Validation score: 0.952976\n",
      "Iteration 24, loss = 0.09748424\n",
      "Validation score: 0.952781\n",
      "Iteration 25, loss = 0.09678402\n",
      "Validation score: 0.953220\n",
      "Iteration 26, loss = 0.09485893\n",
      "Validation score: 0.951902\n",
      "Iteration 27, loss = 0.09380667\n",
      "Validation score: 0.954685\n",
      "Iteration 28, loss = 0.09385602\n",
      "Validation score: 0.953660\n",
      "Iteration 29, loss = 0.09102592\n",
      "Validation score: 0.956248\n",
      "Iteration 30, loss = 0.09094189\n",
      "Validation score: 0.950584\n",
      "Iteration 31, loss = 0.08992779\n",
      "Validation score: 0.952683\n",
      "Iteration 32, loss = 0.09073928\n",
      "Validation score: 0.954881\n",
      "Iteration 33, loss = 0.08900179\n",
      "Validation score: 0.956492\n",
      "Iteration 34, loss = 0.08926507\n",
      "Validation score: 0.955467\n",
      "Iteration 35, loss = 0.08706067\n",
      "Validation score: 0.954392\n",
      "Iteration 36, loss = 0.08758951\n",
      "Validation score: 0.953513\n",
      "Iteration 37, loss = 0.08612034\n",
      "Validation score: 0.956004\n",
      "Iteration 38, loss = 0.08739529\n",
      "Validation score: 0.955662\n",
      "Iteration 39, loss = 0.08546244\n",
      "Validation score: 0.957517\n",
      "Iteration 40, loss = 0.08336346\n",
      "Validation score: 0.955515\n",
      "Iteration 41, loss = 0.08242527\n",
      "Validation score: 0.957664\n",
      "Iteration 42, loss = 0.08279329\n",
      "Validation score: 0.956199\n",
      "Iteration 43, loss = 0.08348436\n",
      "Validation score: 0.955271\n",
      "Iteration 44, loss = 0.08174801\n",
      "Validation score: 0.956101\n",
      "Iteration 45, loss = 0.08231993\n",
      "Validation score: 0.956297\n",
      "Iteration 46, loss = 0.08144484\n",
      "Validation score: 0.955857\n",
      "Iteration 47, loss = 0.08091748\n",
      "Validation score: 0.953855\n",
      "Iteration 48, loss = 0.08313557\n",
      "Validation score: 0.957517\n",
      "Iteration 49, loss = 0.07896770\n",
      "Validation score: 0.955515\n",
      "Iteration 50, loss = 0.07910938\n",
      "Validation score: 0.956297\n",
      "Iteration 51, loss = 0.07875667\n",
      "Validation score: 0.954246\n",
      "Iteration 52, loss = 0.07891466\n",
      "Validation score: 0.955467\n",
      "Validation score did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(early_stopping=True, hidden_layer_sizes=(100, 100, 50),\n",
       "              learning_rate_init=0.008035866825788862, random_state=42,\n",
       "              verbose=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;MLPClassifier<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.neural_network.MLPClassifier.html\">?<span>Documentation for MLPClassifier</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>MLPClassifier(early_stopping=True, hidden_layer_sizes=(100, 100, 50),\n",
       "              learning_rate_init=0.008035866825788862, random_state=42,\n",
       "              verbose=True)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "MLPClassifier(early_stopping=True, hidden_layer_sizes=(100, 100, 50),\n",
       "              learning_rate_init=0.008035866825788862, random_state=42,\n",
       "              verbose=True)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.fit(X_train_final_scaled, y_train_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkAAAAHHCAYAAABXx+fLAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAABcwklEQVR4nO3deVRU9f8/8OedYRb2fQdB3HADERRxSUvccsml0rIk+pilUhotv2xx61tYmVlqmpqabS6VS+WG5JLmCuKCijsoq4jIJjAw9/cHMUWgIjLcGeb5OIcj986dO695McrTe9/3fQVRFEUQERERmRCZ1AUQERERNTYGICIiIjI5DEBERERkchiAiIiIyOQwABEREZHJYQAiIiIik8MARERERCaHAYiIiIhMDgMQERERmRwGIKIm5LnnnoOvr2+9njtz5kwIgtCwBVGT99xzz8HKykrqMojuGwMQUSMQBKFOX7t375a6VEnwl+idPffcc3f8vKjVaqnLIzJaZlIXQGQKvv3222rLq1evRmxsbI31bdu2faDXWbZsGbRabb2e++677+Ktt956oNcn/VCpVFi+fHmN9XK5XIJqiJoGBiCiRvDMM89UWz548CBiY2NrrP+v4uJiWFhY1Pl1FApFveoDADMzM5iZ8Z8EQ2RmZnbPzwoR3R+eAiMyEH369EGHDh0QHx+Phx56CBYWFnj77bcBAJs2bcLgwYPh4eEBlUqFFi1a4P3330dFRUW1ffx3DNCVK1cgCALmzp2LpUuXokWLFlCpVOjSpQuOHDlS7bm1jQESBAFRUVHYuHEjOnToAJVKhfbt22Pbtm016t+9ezdCQkKgVqvRokULfPXVVw0+rmj9+vUIDg6Gubk5nJyc8MwzzyAtLa3aNpmZmYiMjISXlxdUKhXc3d3x2GOP4cqVK7ptjh49igEDBsDJyQnm5uZo3rw5nn/++bu+9pAhQ+Dn51frY2FhYQgJCdEtx8bGomfPnrCzs4OVlRXatGmj+1nqy6pVqyAIAvbu3YsXX3wRjo6OsLGxwbhx43Dz5s0a23/55Zdo3749VCoVPDw8MHnyZOTl5dXY7tChQ3j00Udhb28PS0tLBAQE4PPPP6+xXVpaGoYPHw4rKys4Ozvj9ddfr/H5XLNmDYKDg2FtbQ0bGxt07Nix1n0RNQb+d4/IgNy4cQODBg3CmDFj8Mwzz8DV1RVA5S83KysrREdHw8rKCn/88QemT5+O/Px8fPLJJ/fc7w8//ICCggK8+OKLEAQBH3/8MUaOHIlLly7d86jRvn378Msvv2DSpEmwtrbGF198gVGjRiE1NRWOjo4AgGPHjmHgwIFwd3fHrFmzUFFRgdmzZ8PZ2fnBm/K3VatWITIyEl26dEFMTAyysrLw+eefY//+/Th27Bjs7OwAAKNGjUJSUhJefvll+Pr6Ijs7G7GxsUhNTdUt9+/fH87OznjrrbdgZ2eHK1eu4Jdffrnr648ePRrjxo3DkSNH0KVLF936lJQUHDx4UPdzSEpKwpAhQxAQEIDZs2dDpVLhwoUL2L9//wO9/5ycnBrrlEolbGxsqq2LioqCnZ0dZs6cieTkZCxevBgpKSnYvXu3LozOnDkTs2bNQnh4OCZOnKjb7siRI9i/f7/uMxEbG4shQ4bA3d0dU6ZMgZubG86cOYPffvsNU6ZM0b1mRUUFBgwYgNDQUMydOxc7d+7Ep59+ihYtWmDixIm6fT311FPo27cvPvroIwDAmTNnsH///mr7Imo0IhE1usmTJ4v//evXu3dvEYC4ZMmSGtsXFxfXWPfiiy+KFhYWYklJiW5dRESE6OPjo1u+fPmyCEB0dHQUc3Nzdes3bdokAhB//fVX3boZM2bUqAmAqFQqxQsXLujWHT9+XAQgLliwQLdu6NChooWFhZiWlqZbd/78edHMzKzGPmsTEREhWlpa3vHxsrIy0cXFRezQoYN4+/Zt3frffvtNBCBOnz5dFEVRvHnzpghA/OSTT+64rw0bNogAxCNHjtyzrn+7deuWqFKpxNdee63a+o8//lgUBEFMSUkRRVEUP/vsMxGAeP369fva/51ERESIAGr9GjBggG67lStXigDE4OBgsaysrFp9AMRNmzaJoiiK2dnZolKpFPv37y9WVFTotlu4cKEIQFyxYoUoiqJYXl4uNm/eXPTx8RFv3rxZrSatVlujvtmzZ1fbJigoSAwODtYtT5kyRbSxsRHLy8sfvClEDYCnwIgMiEqlQmRkZI315ubmuu8LCgqQk5ODXr16obi4GGfPnr3nfkePHg17e3vdcq9evQAAly5duudzw8PD0aJFC91yQEAAbGxsdM+tqKjAzp07MXz4cHh4eOi2a9myJQYNGnTP/dfF0aNHkZ2djUmTJlW78mnw4MHw9/fH77//DqCyT0qlErt37671tA8A3ZGi3377DRqNps412NjYYNCgQVi3bh1EUdStX7t2Lbp164ZmzZpV2/+mTZvqPSD9v9RqNWJjY2t8zZkzp8a2EyZMqHZUb+LEiTAzM8OWLVsAADt37kRZWRmmTp0KmeyfXwEvvPACbGxsdL08duwYLl++jKlTp+reU5XaTmu+9NJL1ZZ79epV7fNlZ2eHoqIixMbG3n8DiPSAAYjIgHh6ekKpVNZYn5SUhBEjRsDW1hY2NjZwdnbWDYq9devWPfdb9cu5SlUYulNIuNtzq55f9dzs7Gzcvn0bLVu2rLFdbevqIyUlBQDQpk2bGo/5+/vrHlepVPjoo4+wdetWuLq64qGHHsLHH3+MzMxM3fa9e/fGqFGjMGvWLDg5OeGxxx7DypUrUVpaes86Ro8ejatXr+LAgQMAgIsXLyI+Ph6jR4+utk2PHj0wfvx4uLq6YsyYMVi3bt0DhSG5XI7w8PAaX506daqxbatWraotW1lZwd3dXTcG6k69VCqV8PPz0z1+8eJFAECHDh3uWZ9ara5xuvPfnxEAmDRpElq3bo1BgwbBy8sLzz//fK1jyYgaCwMQkQH595GeKnl5eejduzeOHz+O2bNn49dff0VsbKxuHEVdfrHe6XLpfx/J0MdzpTB16lScO3cOMTExUKvVeO+999C2bVscO3YMQOXRi59++gkHDhxAVFQU0tLS8PzzzyM4OBiFhYV33ffQoUNhYWGBdevWAQDWrVsHmUyGJ554QreNubk59u7di507d+LZZ5/FiRMnMHr0aPTr16/GoOCmoi6X47u4uCAxMRGbN2/GsGHDsGvXLgwaNAgRERGNUCFRTQxARAZu9+7duHHjBlatWoUpU6ZgyJAhCA8Pr3ZKS0ouLi5Qq9W4cOFCjcdqW1cfPj4+AIDk5OQajyUnJ+ser9KiRQu89tpr2LFjB06dOoWysjJ8+umn1bbp1q0bPvjgAxw9ehTff/89kpKSsGbNmrvWYWlpiSFDhmD9+vXQarVYu3YtevXqVe3UHwDIZDL07dsX8+bNw+nTp/HBBx/gjz/+wK5du+rz9u/L+fPnqy0XFhYiIyNDd3XgnXpZVlaGy5cv6x6vOu156tSpBqtNqVRi6NCh+PLLL3Hx4kW8+OKLWL16dYN9TojuBwMQkYGr+t/1v4+4lJWV4csvv5SqpGqqTs9s3LgR6enpuvUXLlzA1q1bG+Q1QkJC4OLigiVLllQ7VbV161acOXMGgwcPBlA5b1JJSUm157Zo0QLW1ta65928ebPG0auqU0l1PQ2Wnp6O5cuX4/jx49VOfwFAbm5ujefUtv+zZ88iNTX1nq93v5YuXVptbNPixYtRXl6uG48VHh4OpVKJL774olofvv76a9y6dUvXy86dO6N58+aYP39+jcvj63P078aNG9WWZTIZAgICANSt70QNjZfBExm47t27w97eHhEREXjllVcgCAK+/fZbgzoFNXPmTOzYsQM9evTAxIkTUVFRgYULF6JDhw5ITEys0z40Gg3+7//+r8Z6BwcHTJo0CR999BEiIyPRu3dvPPXUU7rL4H19ffHqq68CAM6dO4e+ffviySefRLt27WBmZoYNGzYgKysLY8aMAQB88803+PLLLzFixAi0aNECBQUFWLZsGWxsbPDoo4/es85HH30U1tbWeP311yGXyzFq1Khqj8+ePRt79+7F4MGD4ePjg+zsbHz55Zfw8vJCz549ddu1bdsWvXv3rtPtT8rLy/Hdd9/V+tiIESNgaWmpWy4rK9P1IDk5GV9++SV69uyJYcOGAQCcnZ0xbdo0zJo1CwMHDsSwYcN023Xp0kU3tkwmk2Hx4sUYOnQoOnXqhMjISLi7u+Ps2bNISkrC9u3b71n3v40fPx65ubl45JFH4OXlhZSUFCxYsACdOnV64BnQiepFugvQiEzXnS6Db9++fa3b79+/X+zWrZtobm4uenh4iG+++aa4fft2EYC4a9cu3XZ3ugy+tsvCAYgzZszQLd/pMvjJkyfXeK6Pj48YERFRbV1cXJwYFBQkKpVKsUWLFuLy5cvF1157TVSr1Xfowj/udql3ixYtdNutXbtWDAoKElUqlejg4CCOHTtWvHbtmu7xnJwccfLkyaK/v79oaWkp2traiqGhoeK6det02yQkJIhPPfWU2KxZM1GlUokuLi7ikCFDxKNHj96zzipjx44VAYjh4eE1HouLixMfe+wx0cPDQ1QqlaKHh4f41FNPiefOnau2HQCxd+/eD9QbAOLly5dFUfznMvg9e/aIEyZMEO3t7UUrKytx7Nix4o0bN2rsd+HChaK/v7+oUChEV1dXceLEiTUudxdFUdy3b5/Yr18/0draWrS0tBQDAgKqTYFwpykM/vt5+umnn8T+/fuLLi4uolKpFJs1aya++OKLYkZGxj17QKQPgiga0H8jiahJGT58OJKSkmqMS6GGVzVR5JEjR6rNSk1EteMYICJqELdv3662fP78eWzZsgV9+vSRpiAiorvgGCAiahB+fn547rnndHPJLF68GEqlEm+++abUpRER1cAAREQNYuDAgfjxxx+RmZkJlUqFsLAwfPjhhzUm5iMiMgQcA0REREQmR/IxQIsWLYKvry/UajVCQ0Nx+PDhO26blJSEUaNGwdfXF4IgYP78+TW2qaiowHvvvYfmzZvD3NwcLVq0wPvvv29QlwwTERGRtCQNQGvXrkV0dDRmzJiBhIQEBAYGYsCAAcjOzq51++LiYvj5+WHOnDlwc3OrdZuPPvoIixcvxsKFC3HmzBl89NFH+Pjjj7FgwQJ9vhUiIiIyIpKeAgsNDUWXLl2wcOFCAJX3NPL29sbLL7+Mt956667P9fX1xdSpUzF16tRq64cMGQJXV1d8/fXXunWjRo2Cubn5HScS+y+tVov09HRYW1vXetdjIiIiMjyiKKKgoAAeHh6Qye5+jEeyQdBlZWWIj4/HtGnTdOtkMhnCw8N1d1quj+7du2Pp0qU4d+4cWrdujePHj2Pfvn2YN2/eHZ9TWlpabSr2tLQ0tGvXrt41EBERkXSuXr0KLy+vu24jWQDKyclBRUUFXF1dq613dXXF2bNn673ft956C/n5+fD394dcLkdFRQU++OADjB079o7PiYmJwaxZs2qsX758OSwsLOpdCxERETWe4uJijB8/HtbW1vfctsldBr9u3Tp8//33+OGHH9C+fXskJiZi6tSp8PDwQERERK3PmTZtGqKjo3XL+fn58Pb2xvDhw2FjY9Og9Wk0GsTGxqJfv35QKBQNum9ifxsDe6xf7K9+sb/6J2WP8/PzMX78+DoNX5EsADk5OUEulyMrK6va+qysrDsOcK6LN954A2+99ZbuxocdO3ZESkoKYmJi7hiAVCoVVCpVjfUKhUJvPzx97pvY38bAHusX+6tf7K/+SdHj+3k9ya4CUyqVCA4ORlxcnG6dVqtFXFwcwsLC6r3f4uLiGgOf5HI5tFptvfdJRERETYukp8Cio6MRERGBkJAQdO3aFfPnz0dRUREiIyMBAOPGjYOnpydiYmIAVA6cPn36tO77tLQ0JCYmwsrKCi1btgQADB06FB988AGaNWuG9u3b49ixY5g3bx6ef/55ad4kERERGRxJA9Do0aNx/fp1TJ8+HZmZmejUqRO2bdumGxidmppa7WhOeno6goKCdMtz587F3Llz0bt3b+zevRsAsGDBArz33nuYNGkSsrOz4eHhgRdffBHTp09v1PdGREREhkvyQdBRUVGIioqq9bGqUFPF19f3njM6W1tbY/78+bXOEk1EREQEGMCtMIiIiIgaGwMQERERmRwGICIiIjI5DEBERERkchiAiIiIyOQwABEREZHJYQAiIiIik8MA1Ii0WhGZ+SW4USJ1JURERKaNAagR/XgkFb0+2YtfrrDtREREUuJv4kbkaWcOAMgtFSSuhIiIyLQxADUiL/uqACRxIURERCaOAagRefx9BKikQkD+bY3E1RAREZkuBqBGZKE0g72FAgCQlseR0ERERFJhAGpkVafB0vNuS1wJERGR6WIAamQetmoAwDUGICIiIskwADWyqivB0nkKjIiISDIMQI3M8+9TYGk8AkRERCQZBqBG5vn3KTAOgiYiIpIOA1Ajq7oUnkeAiIiIpMMA1Mg87SqPAN0s1qC4rFziaoiIiEwTA1AjszFXwFwuAuCl8ERERFJhAJKAvaryz6s3GYCIiIikwAAkAQdV5RGgNAYgIiIiSTAAScDh7yNAHAhNREQkDQYgCdj/fQToGo8AERERSYIBSAK6I0A3i6UthIiIyEQxAEmg6ggQT4ERERFJgwFIAo5/HwHKLihFaXmFtMUQERGZIAYgCViaAWqFDKIIZPCWGERERI2OAUgCggB42PKWGERERFJhAJJI1S0xOBcQERFR42MAkoinfeURoGu8EoyIiKjRMQBJxNO28gjQNZ4CIyIianQMQBLxsPt7DBBPgRERETU6BiCJeNlzEDQREZFUGIAk4vH3IOiMWyUor9BKXA0REZFpYQCSiIuVCgq5gAqtiKyCUqnLISIiMikMQBKRyQS423IcEBERkRQYgCTkxUvhiYiIJMEAJCFPXglGREQkCQYgCXnySjAiIiJJMABJyMveAgBwjUeAiIiIGhUDkIR0p8B4BIiIiKhRMQBJ6N+TIWq1osTVEBERmQ4GIAm52aohE4Cyci1yijgXEBERUWNhAJKQQi6Dm83fN0XlOCAiIqJGwwAkMd2VYAxAREREjYYBSGIcCE1ERNT4GIAk9s+l8JwNmoiIqLEwAEmMp8CIiIgaHwOQxHgKjIiIqPExAEns30eARJFzARERETUGBiCJVR0BKiqrQF6xRuJqiIiITAMDkMTUCjmcrFQAeBqMiIiosTAAGYCq02CcDJGIiKhxMAAZAC+7qgDES+GJiIgaAwOQAfj3TVGJiIhI/xiADADnAiIiImpckgegRYsWwdfXF2q1GqGhoTh8+PAdt01KSsKoUaPg6+sLQRAwf/78WrdLS0vDM888A0dHR5ibm6Njx444evSont7Bg+NcQERERI1L0gC0du1aREdHY8aMGUhISEBgYCAGDBiA7OzsWrcvLi6Gn58f5syZAzc3t1q3uXnzJnr06AGFQoGtW7fi9OnT+PTTT2Fvb6/Pt/JA/rkdBgMQERFRYzCT8sXnzZuHF154AZGRkQCAJUuW4Pfff8eKFSvw1ltv1di+S5cu6NKlCwDU+jgAfPTRR/D29sbKlSt165o3b66H6htO1SmwW7c1KCwth5VK0h8LERFRkyfZb9qysjLEx8dj2rRpunUymQzh4eE4cOBAvfe7efNmDBgwAE888QT27NkDT09PTJo0CS+88MIdn1NaWorS0lLdcn5+PgBAo9FAo2nYyQmr9vfv/apkgK25GW7dLkfK9Xy0drVu0Nc0JbX1lxoWe6xf7K9+sb/6J2WP7+c1JQtAOTk5qKiogKura7X1rq6uOHv2bL33e+nSJSxevBjR0dF4++23ceTIEbzyyitQKpWIiIio9TkxMTGYNWtWjfU7duyAhYVFvWu5m9jY2GrLVoIctyBg48596GDPW2I8qP/2lxoee6xf7K9+sb/6J0WPi4vrPp1MkzvXotVqERISgg8//BAAEBQUhFOnTmHJkiV3DEDTpk1DdHS0bjk/Px/e3t7o378/bGxsGrQ+jUaD2NhY9OvXDwqFQrf+t7xEpJ3JhkfL9ng0tFmDvqYpuVN/qeGwx/rF/uoX+6t/Uva46gxOXUgWgJycnCCXy5GVlVVtfVZW1h0HONeFu7s72rVrV21d27Zt8fPPP9/xOSqVCiqVqsZ6hUKhtx/ef/ft5VB5pCkzv4x/KRuAPn92VIk91i/2V7/YX/2Tosf383qSXQWmVCoRHByMuLg43TqtVou4uDiEhYXVe789evRAcnJytXXnzp2Dj49PvffZGKouhb/GS+GJiIj0TtJTYNHR0YiIiEBISAi6du2K+fPno6ioSHdV2Lhx4+Dp6YmYmBgAlQOnT58+rfs+LS0NiYmJsLKyQsuWLQEAr776Krp3744PP/wQTz75JA4fPoylS5di6dKl0rzJOuKl8ERERI1H0gA0evRoXL9+HdOnT0dmZiY6deqEbdu26QZGp6amQib75yBVeno6goKCdMtz587F3Llz0bt3b+zevRtA5aXyGzZswLRp0zB79mw0b94c8+fPx9ixYxv1vd0vL84GTURE1GgkHwQdFRWFqKioWh+rCjVVfH19IYr3vkJqyJAhGDJkSEOU12iqToHlFJaiRFMBtUIucUVERERNl+S3wqBKdhYKWCgrQw9viUFERKRfDEAGQhAEngYjIiJqJAxABoQ3RSUiImocDEAGpOqeYNdu1n0mSyIiIrp/DEAGpOpSeJ4CIyIi0i8GIAPCU2BERESNgwHIgHhyEDQREVGjYAAyIF5/HwHKzC+BpkIrcTVERERNFwOQAXGyUkFpJoNWBDJvlUhdDhERUZPFAGRAZDLhn5ui8jQYERGR3jAAGZh/AhAvhSciItIXBiADo5sNmleCERER6Q0DkIFp4WwFADh8OVfiSoiIiJouBiADM7CDGwDgwKUbyLjFo0BERET6wABkYLwdLNC1uQNEEdh4LF3qcoiIiJokBiADNKqzJwDgl4RrEEVR4mqIiIiaHgYgAzSooztUZjKczy5EUnq+1OUQERE1OQxABshGrUB4O1cAwC8JaRJXQ0RE1PQwABmoqtNgm4+noZy3xSAiImpQDEAGqlcrZzhaKpFTWIY/z+dIXQ4REVGTwgBkoBRyGYYGegAAfjnG02BEREQNiQHIgI3q7AUA2JGUifwSjcTVEBERNR0MQAasg6cNWrpYobRci20nM6Uuh4iIqMlgADJggiBgRFDlYOifE65JXA0REVHTwQBk4IYHeUIQgEOXc3mHeCIiogbCAGTgPO3M0a25IwBgUyJvjUFERNQQGICMwIjO/5wG460xiIiIHhwDkBEY1MENaoUMl64X4cS1W1KXQ0REZPQYgIyAtVqB/u3cAAAbOCcQERHRA2MAMhIjdLfGSIeGt8YgIiJ6IAxARqJXSyc4WamQW1SGPcnXpS6HiIjIqDEAGQkzuQyPdaq6NQbnBCIiInoQDEBGpGpSxJ1nsnHrNm+NQUREVF8MQEakvYcN2rhao6xciy0nM6Quh4iIyGgxABkRQRB0g6F/4a0xiIiI6o0ByMg81skDggAcuXITV3N5awwiIqL6YAAyMu625ujRwgkA8EsC5wQiIiKqDwYgIzSy6jTYMd4ag4iIqD4YgIzQwA5usFTKkXKjGEdTbkpdDhERkdFhADJCFkozDOroDgD4OZ6DoYmIiO4XA5CRGtXZCwDw24kM3C6rkLgaIiIi48IAZKRCmzvA084chaXl2HE6U+pyiIiIjAoDkJGSyQSM+nsw9E88DUZERHRfGICM2KjgytNg+y/kIPNWicTVEBERGQ8GICPm42iJLr720IrAhmOcE4iIiKiuGICMXNVg6J8TOCcQERFRXTEAGblHA9yhMpPhQnYhTly7JXU5RERERoEByMjZqBUY0N4NQOVRICIiIro3BqAmoGow9Obj6Sgt55xARERE98IA1AT0bOkEVxsV8oo12HU2W+pyiIiIDB4DUBMglwkYHsQ5gYiIiOqKAaiJePzvq8F2J19HTmGpxNUQEREZNgagJqKVqzUCvGxRrhWxKTFd6nKIiIgMGgNQE6KbE4inwYiIiO6KAagJGRboAYVcwOmMfJzJyJe6HCIiIoPFANSE2Fsq0dffFQCPAhEREd0NA1ATUzUn0MbEdJRXaCWuhoiIyDAxADUxfdo4w9FSiZzCUuw9f13qcoiIiAwSA1ATo5DLMKyTBwDg53jeIZ6IiKg2BhGAFi1aBF9fX6jVaoSGhuLw4cN33DYpKQmjRo2Cr68vBEHA/Pnz77rvOXPmQBAETJ06tWGLNmBVV4PFns5CdkGJxNUQEREZHskD0Nq1axEdHY0ZM2YgISEBgYGBGDBgALKza7+lQ3FxMfz8/DBnzhy4ubnddd9HjhzBV199hYCAAH2UbrDae9igvYcNyiq0GPf1YeQWlUldEhERkUGRPADNmzcPL7zwAiIjI9GuXTssWbIEFhYWWLFiRa3bd+nSBZ988gnGjBkDlUp1x/0WFhZi7NixWLZsGezt7fVVvkESBAGfjwmCs7UKZzML8PSygwxBRERE/2Im5YuXlZUhPj4e06ZN062TyWQIDw/HgQMHHmjfkydPxuDBgxEeHo7/+7//u+u2paWlKC395/YR+fmVc+hoNBpoNJoHquO/qvbX0Pv9Lx97Fb6NDMGzK47gbGYBnlp6AKsjQ+BgqdTr60qtsfprythj/WJ/9Yv91T8pe3w/rylpAMrJyUFFRQVcXV2rrXd1dcXZs2frvd81a9YgISEBR44cqdP2MTExmDVrVo31O3bsgIWFRb3ruJvY2Fi97Pe/XmgJLEySIzmrEMO/2IWodhWwUjTKS0uqsfprythj/WJ/9Yv91T8pelxcXFznbSUNQPpw9epVTJkyBbGxsVCr1XV6zrRp0xAdHa1bzs/Ph7e3N/r37w8bG5sGrU+j0SA2Nhb9+vWDQtE4SaT3Q0V4ZsURZBSWYfVVO3wTGQLHJnokSIr+mhr2WL/YX/1if/VPyh5XncGpC0kDkJOTE+RyObKysqqtz8rKuucA5zuJj49HdnY2OnfurFtXUVGBvXv3YuHChSgtLYVcLq/2HJVKVet4IoVCobcfnj73/V9tPOyw5sUwjFl6EMlZhXhuVTy+Hx8KR6s7j6Eydo3ZX1PFHusX+6tf7K/+SdHj+3k9SQdBK5VKBAcHIy4uTrdOq9UiLi4OYWFh9dpn3759cfLkSSQmJuq+QkJCMHbsWCQmJtYIP6aihbMV1kzoBpe/B0aPXX4INwpL7/1EIiKiJkjyU2DR0dGIiIhASEgIunbtivnz56OoqAiRkZEAgHHjxsHT0xMxMTEAKgdOnz59Wvd9WloaEhMTYWVlhZYtW8La2hodOnSo9hqWlpZwdHSssd7UtHC2wo8TuuGppQd1IaipHwkiIiKqjeSXwY8ePRpz587F9OnT0alTJyQmJmLbtm26gdGpqanIyMjQbZ+eno6goCAEBQUhIyMDc+fORVBQEMaPHy/VWzAqVSHo30eCSjQVUpdFRETUqCQ/AgQAUVFRiIqKqvWx3bt3V1v29fWFKIr3tf//7sPUVYWg0V8dwNnMAnx3MAXje/lJXRYREVGjkfwIEEmjhbMV3hzoDwBY8McF3CrmnBhERGQ6GIBM2KjOXmjjao1btzX4cvcFqcshIiJqNAxAJkwuE/DWo5VHgVb+dQXXbtZ9AikiIiJjxgBk4vq0dkaYnyPKyrWYt+Oc1OUQERE1CgYgEycIAqb9fRRoQ2IaktJvSVwRERGR/jEAEQK87DAs0AOiCMzZWv97sBERERkLBiACALwxoA0UcgF/ns/B3nPXpS6HiIhIrxiACADg7WCBZ7v5AgBitp6FVnt/cy0REREZEwYg0nn5kZawVpvhTEY+NiamSV0OERGR3jAAkY69pRKT+rQEAMzdnsxbZBARUZPFAETVRPbwhbutGum3SvDNX1ekLoeIiEgvGICoGrVCjuh+rQEAi3ZdQF5xmcQVERERNTwGIKphZGcv+LtZI7+kHIt28RYZRETU9DAAUQ1ymYC3BlVOjvjNXym4mstbZBARUdPCAES16t3aGd1bOKKsQou5O5KlLoeIiKhBMQBRrQRBwLRBbQEAmxLTceDiDYkrIiIiajgMQHRHHb1s8XRoMwDAOxtPorScl8UTEVHTwABEd/X/BvrDyUqFS9eLsHj3RanLISIiahAMQHRXtuYKzBjaDgDw5a6LuHi9UOKKiIiIHhwDEN3TkAB39G7tjLIKLd7ZcBKiyPuEERGRcWMAonsSBAH/N7wD1AoZDl7Kxc8JvE8YEREZNwYgqhNvBwtMDa+cIfqD308jt4gzRBMRkfFiAKI6+1/P5vB3s8bNYg0++P2M1OUQERHVGwMQ1ZlCLsOHIztCEICfE67hr4s5UpdERERULwxAdF86N7PHM6E+AIB3N5xCiYZzAxERkfFhAKL79sbANnC2VuFSDucGIiIi41SvAHT16lVcu3ZNt3z48GFMnToVS5cubbDCyHDZqBWYObQ9AGDx7ou4kM25gYiIyLjUKwA9/fTT2LVrFwAgMzMT/fr1w+HDh/HOO+9g9uzZDVogGaZHO7rhEX8Xzg1ERERGqV4B6NSpU+jatSsAYN26dejQoQP++usvfP/991i1alVD1kcGShAEzBrWHuYKOQ5dzsW6o1elLomIiKjO6hWANBoNVCoVAGDnzp0YNmwYAMDf3x8ZGRkNVx0ZNG8HC7zarxUA4L2NSfjz/HWJKyIiIqqbegWg9u3bY8mSJfjzzz8RGxuLgQMHAgDS09Ph6OjYoAWSYXu+R3MM6uCGsgotXlh9FIcu3ZC6JCIionuqVwD66KOP8NVXX6FPnz546qmnEBgYCADYvHmz7tQYmQYzuQyfjwnCw22cUaLR4vlVR3As9abUZREREd2VWX2e1KdPH+Tk5CA/Px/29va69RMmTICFhUWDFUfGQWkmw+JngvH8qiP46+INRKw4jB8ndEN7D1upSyMiIqpVvY4A3b59G6Wlpbrwk5KSgvnz5yM5ORkuLi4NWiAZB7VCjmXjQhDiY4/8knI8+/VhnM8qkLosIiKiWtUrAD322GNYvXo1ACAvLw+hoaH49NNPMXz4cCxevLhBCyTjYakyw4rILgjwskVuURnGLj+EKzlFUpdFRERUQ70CUEJCAnr16gUA+Omnn+Dq6oqUlBSsXr0aX3zxRYMWSMbFRq3AN5Fd4e9mjeyCUoxdfgjXbhZLXRYREVE19QpAxcXFsLa2BgDs2LEDI0eOhEwmQ7du3ZCSktKgBZLxsbdU4tv/hcLP2RJpebcxdvkhZOWXSF0WERGRTr0CUMuWLbFx40ZcvXoV27dvR//+/QEA2dnZsLGxadACyTg5W6vww/hu8HYwR8qNYjy97CByCkulLouIiAhAPQPQ9OnT8frrr8PX1xddu3ZFWFgYgMqjQUFBQQ1aIBkvN1s1fhjfDe62aly8XoQJq4+irFwrdVlERET1C0CPP/44UlNTcfToUWzfvl23vm/fvvjss88arDgyft4OFvhufCis1WZISM3D+7+dlrokIiKi+gUgAHBzc0NQUBDS09N1d4bv2rUr/P39G6w4ahpaOFth/uhOAIBvD6ZgPe8bRkREEqtXANJqtZg9ezZsbW3h4+MDHx8f2NnZ4f3334dWy1McVFPftq6YGl5537B3Np7CyWu3JK6IiIhMWb0C0DvvvIOFCxdizpw5OHbsGI4dO4YPP/wQCxYswHvvvdfQNVIT8cojrdDX3wVl5Vq89F08bnBQNBERSaReAeibb77B8uXLMXHiRAQEBCAgIACTJk3CsmXLsGrVqgYukZoKmUzAvNGd0Nyp8vL4V9YcQ3kFjxgSEVHjq1cAys3NrXWsj7+/P3Jzcx+4KGq6bM0V+OrZYFgo5dh/4QY+2Z4sdUlERGSC6hWAAgMDsXDhwhrrFy5ciICAgAcuipq21q7W+OTxQADAV3sv4bcT6RJXREREpqZed4P/+OOPMXjwYOzcuVM3B9CBAwdw9epVbNmypUELpKZpcIA7Tlzzw1d7L+HNn06glYs12rhZS10WERGZiHodAerduzfOnTuHESNGIC8vD3l5eRg5ciSSkpLw7bffNnSN1ES9MaANerR0RHFZBV789ihu3dZIXRIREZmIeh0BAgAPDw988MEH1dYdP34cX3/9NZYuXfrAhVHTZyaXYcFTnTF0wT5cuVGM6LWJWDYuBDKZIHVpRETUxNV7IkSihuBgqcSSZ4KhNJMh7mw2/u/3MxBFUeqyiIioiWMAIsl19LLFx6MqB8+v2H8Zn+44J3FFRETU1DEAkUEYHuSJ9x9rDwBYuOsCFv5xXuKKiIioKbuvMUAjR4686+N5eXkPUguZuGfDfFGi0eKDLWcwd8c5qBVyjO/lJ3VZRETUBN1XALK1tb3n4+PGjXuggsi0vfCQH25rKjAv9hz+7/czUCvkeKabj9RlERFRE3NfAWjlypX6qoNI5+VHWuK2pgKLd1/EuxtPQa2Q4/FgL6nLIiKiJoRjgMjgCIKANwe0wXPdfQEAb/50nLNFExFRg2IAIoMkCAKmD2mHMV28oRWBqWsSEXs6S+qyiIioiWAAIoMlkwn4YERHDO/kgXKtiMnfJ2DvuetSl0VERE2AQQSgRYsWwdfXF2q1GqGhoTh8+PAdt01KSsKoUaPg6+sLQRAwf/78GtvExMSgS5cusLa2houLC4YPH47kZN513BjJZQLmPhGIQR3cUFahxYRvj2L90aucLJGIiB6I5AFo7dq1iI6OxowZM5CQkIDAwEAMGDAA2dnZtW5fXFwMPz8/zJkzB25ubrVus2fPHkyePBkHDx5EbGwsNBoN+vfvj6KiIn2+FdITM7kMn48JQnhbF5RotHjjpxOI+uEY8orLpC6NiIiMlOQBaN68eXjhhRcQGRmJdu3aYcmSJbCwsMCKFStq3b5Lly745JNPMGbMGKhUqlq32bZtG5577jm0b98egYGBWLVqFVJTUxEfH6/Pt0J6pDST4atnQ/DmwDYwkwn4/WQGBs7/E39dzJG6NCIiMkL1vhlqQygrK0N8fDymTZumWyeTyRAeHo4DBw402OvcunULAODg4FDr46WlpSgtLdUt5+fnAwA0Gg00moa9Q3nV/hp6v6bihR4+6OZrh9fWn8TlG8UYu/wQ/tfDF6/2bQmlmYz9bQTssX6xv/rF/uqflD2+n9eUNADl5OSgoqICrq6u1da7urri7NmzDfIaWq0WU6dORY8ePdChQ4dat4mJicGsWbNqrN+xYwcsLCwapI7/io2N1ct+TcXEFsBGuQx/ZcuwfN8VbDt2Gc+2rIDb3z8u9lf/2GP9Yn/1i/3VPyl6XFxcXOdtJQ1AjWHy5Mk4deoU9u3bd8dtpk2bhujoaN1yfn4+vL290b9/f9jY2DRoPRqNBrGxsejXrx8UCkWD7tvUjACw80w23t6YhGtFGnx2Wok3+7WEQ+5p9O/P/uoLP8P6xf7qF/urf1L2uOoMTl1IGoCcnJwgl8uRlVV9fpesrKw7DnC+H1FRUfjtt9+wd+9eeHndeSZhlUpV63gihUKhtx+ePvdtSgYFeKKzryNeX38cf57Pwewt59DRXob+A+Xsr57xM6xf7K9+sb/6J0WP7+f1JB0ErVQqERwcjLi4ON06rVaLuLg4hIWF1Xu/oigiKioKGzZswB9//IHmzZs3RLlkoFxt1PgmsiveG9IOCrmAkzdl+O7QVanLIiIiAyb5KbDo6GhEREQgJCQEXbt2xfz581FUVITIyEgAwLhx4+Dp6YmYmBgAlQOnT58+rfs+LS0NiYmJsLKyQsuWLQFUnvb64YcfsGnTJlhbWyMzMxNA5c1azc3NJXiXpG8ymYD/9WwOtRx4Z9NpfPHHRQzv7AUXa7XUpRERkQGS/DL40aNHY+7cuZg+fTo6deqExMREbNu2TTcwOjU1FRkZGbrt09PTERQUhKCgIGRkZGDu3LkICgrC+PHjddssXrwYt27dQp8+feDu7q77Wrt2baO/P2pcj3f2hLeliMLScny0lZNfEhFR7SQ/AgRUjtWJioqq9bHdu3dXW/b19b3nLMCcJdh0yWQCHm9egc9OmeHnhGt4OrQZgn3spS6LiIgMjORHgIgamq915ZEgAJix+RQqtAzERERUHQMQNUmv92sJa7UZTqXlY+0RDogmIqLqGICoSXK0UiG6X2sAwMfbz+JmEe8bRkRE/2AAoibr2W4+aONqjbxiDT6N5YBoIiL6BwMQNVlmchlmDmsPAPjhUCpOpd2SuCIiIjIUDEDUpIW1cMTQQA9oRWDm5iReIUhERAAYgMgEvP2oP8wVchxNuYmNiWlSl0NERAaAAYiaPHdbc7zct3KW8A+3nEVBiUbiioiISGoMQGQS/tezOZo7WeJ6QSm+iDsvdTlERCQxBiAyCSozOaYPbQcAWLn/Ci5kF0hcERERSckgboVB1BgebuOC8Lau2HkmCxNWx+Nhfxf4u1mjrbsNWrpYQa2QS10iERE1EgYgMinTh7TDwUs3cCmnCJf2XdatlwlAcydL+LvboK2bNfzdbNCzlRNDERFRE8UARCalmaMFdrz6EP48fx1nMgpwNjMfZzMLkFeswcXrRbh4vQi/n8gAALRxtcZPE8NgrVZIXDURETU0BiAyOR525hjdpZluWRRFZBeU4kxGPpIzC3A2swC7k7ORnFWAKWsSsWxcCOQyQcKKiYiooTEAkckTBAGuNmq42qjRp40LAODEtTw8seQA/jibjY+3ncW0R9tKXCURETUkXgVGVIsALzt88kQgAOCrvZfwc/w1iSsiIqKGxABEdAfDAj0Q9XDlBIrTfjmJhNSbEldEREQNhQGI6C6i+7VG/3auKKvQYsLqeKTn3Za6JCIiagAMQER3IZMJ+Gx0J/i7WSOnsBQTvj2K22UVUpdFREQPiAGI6B4sVWZYNi4EDpZKnErLx+s/Hedd5YmIjBwDEFEdeDtYYMkzwVDIBfx+IgML/rggdUlERPQAGICI6qhrcwe8/1gHAMC82HPYejJD4oqIiKi+GICI7sOYrs0Q2cMXABC97jiOXMmVtiAiIqoXBiCi+/TOo23Rq5UTbmsq8MSSA5j4XTwuZBdKXRYREd0HBiCi+2Qml2HR2M4Y2dkTggBsPZWJ/p/twRvrjyONl8kTERkFBiCierBRKzDvyU7YNuUh9GvnCq0IrI+/hoc/2Y3Zv57GjcJSqUskIqK7YAAiegBt3KyxbFwIfpnUHd38HFBWocWK/Zfx0Me7MC/2HApKNFKXSEREtWAAImoAnZvZ48cXumH1813R0dMWRWUV+CLuPB76eBd2JWdLXR4REf0HAxBRAxEEAQ+1dsbmqB74cmxn+Dlb4maxBhNWH8WOpEypyyMion9hACJqYIIg4NGO7tg+9SEM7ugOTYWISd8ncN4gIiIDwgBEpCcKuQyfj+mExzp5oFwrIurHY/j1eLrUZRERERiAiPTKTC7DvCc7YWRnT1RoRUxZcwwbjl2TuiwiIpPHAESkZ3KZgE8eD8STIV7QipUzSK8/elXqsoiITBoDEFEjkMsEzBkZgKdDm0EUgTd/PoEfD6dKXRYRkcliACJqJDKZgA+Gd0BEmA9EEZj2y0l8e+CK1GUREZkkBiCiRiQIAmYOa4//9WwOAHhvUxJW7r8scVVERKaHAYiokQmCgHcHt8WLvf0AALN+PY3odYm4WVQmcWVERKaDAYhIAoIg4K2B/ng1vDUEAfglIQ3h8/Zg8/F0iKIodXlERE0eAxCRRARBwJTwVvh5Yne0drXCjaIyvPLjMYz/5ijSeVd5IiK9YgAikljnZvb47eVeiO7XGkq5DHFns9Fv3h6sPnAFWi2PBhER6QMDEJEBUJrJ8ErfVvj9lZ4I9rFHUVkFpm9KwhNfHcD5rAKpyyMianIYgIgMSCtXa6x/MQzvP9Yelko54lNu4tEv/sScrWcRn5KLsnKt1CUSETUJZlIXQETVyWQCng3zRd+2rnhv4ynEnc3Gkj0XsWTPRagVMnTytkPX5o7o6uuAzj52sFDyrzER0f3iv5xEBsrDzhzLI0Kw7VQmNiam4ciVm8gtKsPBS7k4eCkXQOUM0x08bdHV1x49WzmjRwtHmMl5YJeI6F4YgIgMmCAIGNTRHYM6ukMURVy8XojDl2/i8OUbOHLlJtLybuP41Twcv5qHZX9ehpOVEkMDPTC8kycCvGwhCILUb4GIyCAxABEZCUEQ0NLFGi1drPF0aDMAwLWbxThyJReHLuVix+ks5BSWYeX+K1i5/wr8nCwxPMgTwzt5opmjhcTVExEZFgYgIiPmZW8BL3sLjAjywvvDtfjz/HVsPJaOHaczcSmnCPNiz2Fe7Dl0bmaHEUGeGNbJE7bmCqnLJiKSHAMQUROhkMvwiL8rHvF3RWFpObb/PXZo/4UcJKTmISE1D4t3X8Tq/3VFSxdrqcslIpIUR0sSNUFWKjOMCvbCt/8LxcFpffHu4LbwdjBH+q0SPL7kAI6l3pS6RCIiSTEAETVxLjZqjO/lh02TeyLQ2w55xRo8vewQ9py7LnVpRESSYQAiMhEOlkr8MD4UvVo54bamAv9bdQSbEtOkLouISBIMQEQmxFJlhq8jumBYoAfKtSKmrEnEyv2XpS6LiKjRMQARmRilmQzzR3fCc919AQCzfj2NT7afhSjyxqtEZDoYgIhMkEwmYMbQdni9f2sAwKJdFzHtl5Mor+C9xojINDAAEZkoQRAQ9UgrxIzsCJkArDlyFZO+T0CJpkLq0oiI9I4BiMjEPdW1Gb4c2xlKuQw7Tmdh1OK/kJxZIHVZRER6xQBERBjYwR2rnu8COwsFktLzMXTBPny15yIqtBwXRERNEwMQEQEAurdwwo6pD+ERfxeUVWgRs/UsRn91AFdyiqQujYiowTEAEZGOi40aX0eE4ONRAbBSmeFoyk0M+vxPfHvgCrQ8GkRETYhBBKBFixbB19cXarUaoaGhOHz48B23TUpKwqhRo+Dr6wtBEDB//vwH3icR/UMQBDzZxRvbpvZCmJ8jbmsq8N6mJIxbcRjpebelLo+IqEFIHoDWrl2L6OhozJgxAwkJCQgMDMSAAQOQnZ1d6/bFxcXw8/PDnDlz4Obm1iD7JKKavOwt8P34UMwc2g5qhQz7LuRgwGd78XNCGjhlEBEZO8kD0Lx58/DCCy8gMjIS7dq1w5IlS2BhYYEVK1bUun2XLl3wySefYMyYMVCpVA2yTyKqnUwm4LkezbHllV4IamaHgtJyvLUhCTHH5Vj652Vk55dIXSIRUb2YSfniZWVliI+Px7Rp03TrZDIZwsPDceDAgUbbZ2lpKUpLS3XL+fn5AACNRgONRlOvOu6kan8NvV+qxP7qh7edCj/+rwuW77uCRbsvIuu2Fp/sOI95Oy/goVaOeLyzJ/q0dobSTPL/Uxk9fob1i/3VPyl7fD+vKWkAysnJQUVFBVxdXautd3V1xdmzZxttnzExMZg1a1aN9Tt27ICFhUW96riX2NhYveyXKrG/+uENYGYn4NgNAYeuy3C5ANiVnINdyTmwNBPRxVlEqLMWHpZSV2r8+BnWL/ZX/6TocXFxcZ23lTQAGYpp06YhOjpat5yfnw9vb2/0798fNjY2DfpaGo0GsbGx6NevHxQKRYPum9jfxqDRaKCOjcX0Zx5Bal4Zfk5Iw8bEdFwvLMPuDAG7M2Ro7WIFZ2sVLJRyWCjlMFfKYaGo/NNcIYelSg5ve3M81MoJgiBI/ZYMCj/D+sX+6p+UPa46g1MXkgYgJycnyOVyZGVlVVuflZV1xwHO+tinSqWqdTyRQqHQ2w9Pn/sm9rcxKBQK+HtY4B0PO/y/QW2x59x1rD96DTvPZOFcdiHOZRfecx992jjj41EBcLFRN0LFxoWfYf1if/VPih7fz+tJGoCUSiWCg4MRFxeH4cOHAwC0Wi3i4uIQFRVlMPskorszk8vQt60r+rZ1xY3CUhxNuYmi0nIUl1XgdlkFissqUFxW/vefFSgqLceu5GzsTr6O/vP34sMRHfFoR3ep3wYRmRDJT4FFR0cjIiICISEh6Nq1K+bPn4+ioiJERkYCAMaNGwdPT0/ExMQAqBzkfPr0ad33aWlpSExMhJWVFVq2bFmnfRKR/jhaqTCg/b2P4J7PKsCr6xJxKi0fk75PwMggT8x8rD1s1PxfORHpn+QBaPTo0bh+/TqmT5+OzMxMdOrUCdu2bdMNYk5NTYVM9s+VJenp6QgKCtItz507F3PnzkXv3r2xe/fuOu2TiKTXytUav0zsgS/izuPL3Rfwy7E0HLx0A3OfDET3Fk5Sl0dETZzkAQgAoqKi7nh6qirUVPH19YVYh1nY7rZPIjIMSjMZXh/QBg/7uyB6XSJSbhTj6WWH8L+ezfHGgDZQK+RSl0hETRQn7SAiyQX72GPLK73wdGgzAMDX+y5j2MJ9iE+5Waf/8BAR3S+DOAJERGSpMsOHIzoivK0L3vzpJM5lFWLU4r/gZqNGr1ZO6NXaGT1bOsHBUil1qUTUBDAAEZFBecTfFTtetcf7v53GlpMZyMwvwfr4a1gffw2CAHT0tEWvVk54qJUzgprZc/ZpIqoXBiAiMjgOlkp8NroTYkZ2xJErufjzfA72nruOs5kFOHHtFk5cu4VFuy7CUinHiM6eeOfRdjBXcrwQEdUdAxARGSy1Qo5erZzRq5Uz3n60LbLyS/Dn+Rz8ef46/jyfg9yiMnx3MBVHLt/EorGd0dLFSuqSichI8NgxERkNVxs1Hg/2wudjgnD0nXCsiuwCZ2sVkrMKMGzhPmxKTJO6RCIyEgxARGSUZDIBfdq44PdXeiLMzxHFZRWYsiYRb284iRJNhdTlEZGBYwAiIqPmYq3Gd+ND8cojLSEIwA+HUjHyy79wJadI6tKIyIAxABGR0ZPLBET3b4NVkV3hYKnE6Yx8DF2wD1tPZkhdGhEZKAYgImoyerd2xu+v9ESIjz0KSssx8fsEzPo1CWXlWqlLIyIDwwBERE2Ku605fpzQDS/29gMArNx/BX0+2YVFuy7gekGpxNURkaFgACKiJkchl2HaoLZYPi4EjpZKpN8qwSfbk9F9Thxe+fEYDl/O5S02iEwc5wEioiYrvJ0r9rd6BFtOZuDbgyk4lpqHzcfTsfl4Otq4WuOZMB+MCPKElYr/FBKZGv6tJ6ImTa2QY2RnL4zs7IVTabfw3cEUbExMQ3JWAd7beApztpzBY0GecLdRQ6MVUV6hRblWhKZCi/IKEeVaLTQVImzNFZjwkB9cbdRSvyUiagAMQERkMjp42mLOqABMG9QWPydcw3cHU3Appwg/HEqt0/P3nLuOn14Kg50Fb8hKZOwYgIjI5NhaKPB8z+aI7OGLvy7ewNZTGajQijCTyWAmF6CQy2AmE2Aml0EhEyCXC1j9VwouZBdi/DdH8d34UKgVvPcYkTFjACIikyUIAnq0dEKPlk733PYRfxc8seQAjqbcxMs/HsPisZ1hJud1JETGin97iYjqwN/NBsvGhUBpJkPs6Sy8tymJV5IRGTEGICKiOurm54jPR3eCIAA/Hk7FF3EXpC6JiOqJAYiI6D4M6uiO2cPaAwA+23kOPx6u2wBqIjIsDEBERPfp2TBfRD3cEgDwzoaTiD2dJXFFRHS/GICIiOrhtf6t8WSIF7QiEPVDAuJTcqUuiYjuAwMQEVE9CIKAD0d0xCP+Ligt1+J/3xzFhewCqcsiojriZfBERPVkJpdh4dNBeHrZISRezcOTXx1ESxcrqMxkUCvk1f5UmcmhVsjgZW+BwQHusDVXSF0+kUljACIiegAWSjOseK4LHl/yFy5dL8Lhy/c+FTb7tyQM7uiBp0OboXMzOwiCUKfXuppbjO1JmTh46QYCvOzwYm8/qMw4ISNRfTAAERE9IAdLJX5/uRcOXMrB7TItSjQVKC2v/c8DF28gOasAPydcw88J19DG1RpPdfXGiCAv2FpUPyokiiIuZBdi26lMbD+diVNp+brHdp7JxpaTGZj3ZCe087Bp7LdMZPQYgIiIGoC5Uo5H/F3vuZ0oikhIzcOPh1Px24l0JGcVYOavpxGz9SyGBHjgyWAPpBYCn8aex44z2bh0vUj3XJkAdPF1QGhzB3x3KBVnMwvw2KJ9mBreGi8+5MeZqYnuAwMQEVEjEgQBwT72CPaxx3tD2mFTYhp++DvMVB0Vqvyn+TIAQCmXoUdLRwzs4Ibwtq5wtFIBqLwU/+2/L8H/ZHsydp7JwqdPBMLP2Uq6N0dkRBiAiIgkYmuuwLgwXzzbzQeJVyuPCv16PB0VFRXo29YNgwI88HAbZ1iraw6YdrZWYemzwfglIQ0zNyfhWGoeHv3iT0wb1BbPdvOBTFa3cUVEpooBiIhIYoIgIKiZPYKa2WPGYH9s27YNw4YEQqG4+5VigiBgVLAXwlo44s2fTmDfhRzM2JyEHacz8fHjgfC0M4coiigoLUd2fgkyb5UiK78EmfklyM4vQblWxHPdfdHK1bqR3imR4WAAIiIyIEozGczucyiPh505Vj/fFd8dSsGHW85g/4UbGPDZXjhbq5CVX4Lisoo7PndzYjqWPBuMHi2dHrByIuPCAERE1ATIZALGhfmiVytnvLYuEQmpeSgsLdc9bqM2g6uNGm62arhYq+Fmq8KhS7k4mnITESsOI2ZkRzwR4i3hOyBqXAxARERNSHMnS6x/qTsOXb4BuSDA1UYNVxs1zJU15wsq0VTgjZ9O4Nfj6XjjpxO4evM2Xg1vVed5iYiMGQMQEVETI5cJ6N7i3qe01Ao5Ph/dCc0czLFo10V8EXce13KLMWdUAJT3ex6OyMjwE05EZMJkMgFvDPDHnJEdIZcJ+OVYGsatOIRbxRqpSyPSKx4BIiIijOnaDO525pj8fQIOXsrFyMX7sSqyK7wdLGpsm3mrBMev5eH41TycuHYLCrmAqEdaIdjHvt6vn1tUBiuVGY88UaNhACIiIgBA79bOWP9SGJ5fdQQXrxdhxJf78cWYIJRrRZy4lofEq7dw4loesgtKazx3V/J1DO/kgf83yB/utuZ1fs0rOUVY8McFbExMg6edOb4c2xkdPG0b8m0R1YoBiIiIdNq622DDpB54ftURnM7Ix9PLD9XYRi4T0MrFCp287RDgZYcT1/Kw9uhVbExMx/akLEzq0wIvPOQHteLON2pNuVEZfDYcS0OFVgQApOYWY+TivzBjaDs83bUZB2OTXjEAERFRNW62aqx7KQyvrUtE7OksNHOwQICXHQK8bNHJ2w7tPWyrXVX2dGgzPNPNB7N+TcKRKzfxaew5rDlyFe8MbotBHdyqBZmUG0VY+McF/PKv4PNwG2eM7+WHlfsvY+eZbLyz4RSOXM7FByM6wlLFX1OkH/xkERFRDVYqM3z1bAg0FVoo6nCT1Q6etlj3Yhh+PZGBmC1nkJZ3G5O+T0BocwdMH9oO1ioFFu46j58T/gk+fdo4Y0rfVghqVjl2KMzPEcv+vISPtydjY2I6TqXnY/HYzpypmvSCAYiIiO6oLuGniiAIGBbogX5tXbFkz0Us2XMRhy7nYsiCfZAJwh2DTxWZTMCLvVsgqJk9Xv4xAReyCzFs4X58OLIDRgR5Nej7IuJweyIialDmSjle7dcaf7zeB0MC3CGKQIVWRO/WzvhlUnesiuxaI/z8W9fmDvj9lV7o0dIRtzUVeHXtcUz75SRKNHe+pQfR/eIRICIi0gtPO3MsfLozXup9CwDu6+ouJysVVj8fii/izuOLP87jx8OpOH41D4uf6QwfR0t9lUwmhEeAiIhIrzp42tbr0na5TMCr/Vrjm8iucLBU4nRGPh5btB9/XcjRQ5VkahiAiIjIoD3U2hlbXumFQG875BVr8OyKw/j2wBWpyyIjxwBEREQGz81WjbUTumFEkCcqtCLe25SEdzachKZCK3VpZKQYgIiIyCioFXLMezIQ/2+gPwQB+P5QKp79+hBuFpVJXRoZIQYgIiIyGoIgYGKfFlj2bAgslXIcvJSLxxbtx7msAqlLIyPDAEREREYnvJ0rfpnUA94O5pW30PjyL8SdyZK6LDIiDEBERGSU2rhZY9Pknght7oDC0nKMX30Ui3dfhCiKentNrVZEUvotLNt7CT/HX9NN7kjGh/MAERGR0XKwVOLb/4Vi5q9J+OFQKj7adhbr469iVGcvDA/yhKdd3e9MfydZ+SX483wO/jx/Hfsv5CCn8J8xRz8cTsWnTwTC18mw5ybKLSpD2s3b6Oh1/9MRNFUMQEREZNSUZjJ8MLwD2rpZ48MtZ3HpehE+2Z6MuTuS0a25I0YFe2FQB7c631j1ZlEZEq/lYd/foedcVmG1xy2UcnTxdUBCyk3Ep9zEoM//xLRH/fFMqA9kMsO7g/2x1JsY/81R3Cgqw4KngjA00EPqkgwCAxARERk9QRDwbJgvRnT2wtaTGfg54RoOXsrFgUs3cODSDby38RQGdXDDsEA3aEUg/7YG1zKLcOVGES7nFOFKThEu3yjGlZwi3Lqt+c++gQBPW/Rq5YyerZzQuZk9lGYypOXdxhvrj+OvizcwfVMSdiRl4ePHA+DRAEedGsr2pExMWXMMJZrK6QJm/3Yavds4w0atkLgy6TEAERFRk2GlMsMTId54IsQb124WY0NCGn45lobLOUX45Vjl9wqZHJqDu+66H28Hc/Ro4YRerZzRvYUj7C2VNbbxtDPHd/8LxeoDVzBn21nsu5CDAfP3YubQ9hjZ2ROCcOejQQUlGiRezcPp9Hz4OVvhodZOUJnJH/j9/9vK/Zcx+7fTEEXg4TbOuHKjGJdzijBvxznMHNa+QV/LGDEAERFRk+Rlb4GX+7ZC1CMtkZCah18SruHX4+nILykHUHm/seZOFvB1tERzZ0s0d7SEr5MlfBwtYKGs269HmUzAcz2a46HWzohedxyJV/Pw2vrj2J6UiQ9HdoSTlQqiKOLKjeLKU2apN5GQchPJWQX491hta7UZBrZ3w9BAD3Rv4Qgzef2vUarQivjg9zNYsf8yAODp0GaYPaw9Dly6gWe/PozVB67g8WCvet2epClhACIioiZNEAQE+9gj2Mce0wa0wvebtuOJIf3hYN1wp6r8nK3w00th+GrvJczfeQ47TmfhaMpNdG5mh4TUPOTWMlmjt4M52rnbIPFqHrLyS7E+/hrWx1+Do6USgzq6YVigJ0J87O9rXFGJpgJT1yRiW1ImAOD/DfTHS739IAgCerVyxpAAd/x2IgPvbjyFXyZ2N8gxS42FAYiIiEyGSiGHm0XlEZeGZiaXYfLDLfFwGxdEr0vE2cwC7DyTDQBQymXo6GWLYB97dG5mj84+dnCxVgOovLT+yJVcbD6ejq2nMnGjqAzfHUzFdwdT4WajxuAAd4T5OSLA21b3nNrcKCzF+NVHcSw1D0q5DHOfDMSw/wx4fm9IO+xOvo7Eq3lYe/QqnurarMH7YCwYgIiIiBpQOw8bbIrqgZ/j01BcVo7OPvZo72FzxzE+MpmAUD9HhPo5Yuaw9vjr4g38ejwd209lIjO/BF/vu4yv91WezvKwVSPAyw4B3rYI9LJDRy9b2KgVuHS9EJGrjiDlRjFszRVY+mwwQv0ca7yWq40ar/Zrjfd/O405W8+ifztXOFqp9NoPQ2UQEyEuWrQIvr6+UKvVCA0NxeHDh++6/fr16+Hv7w+1Wo2OHTtiy5Yt1R4vLCxEVFQUvLy8YG5ujnbt2mHJkiX6fAtEREQ6KjM5ng5thvG9/NC5mX2dBzgr5DL0bu2MuU8E4si74Vj6bDAeD/ZCKxcrCAKQfqsE25Iy8fG2ZIxdfggBM3fgkbm7MeLLv5ByoxjeDub4eWL3WsNPlYgwH/i7WePWbQ0+2na2od6y0ZH8CNDatWsRHR2NJUuWIDQ0FPPnz8eAAQOQnJwMFxeXGtv/9ddfeOqppxATE4MhQ4bghx9+wPDhw5GQkIAOHToAAKKjo/HHH3/gu+++g6+vL3bs2IFJkybBw8MDw4YNa+y3SEREdN/UCjn6t3dD//ZuAIDC0nKcSruFE9fycPxa5Z9Xc2/jUk4RACDQyxbLI7rA2fruR3TM5DJ8MKIDRi0+gHVHr+HJEG+E+Dro/f0YGsmPAM2bNw8vvPACIiMjdUdqLCwssGLFilq3//zzzzFw4EC88cYbaNu2Ld5//3107twZCxcu1G3z119/ISIiAn369IGvry8mTJiAwMDAex5ZIiIiMlRWKjN083PEhIdaYNHTnfHnm48g4b1+WBnZBQufDsKaCWH3DD9Vgn0cMDrEGwDw7sZTKK/Q6rN0gyRpACorK0N8fDzCw8N162QyGcLDw3HgwIFan3PgwIFq2wPAgAEDqm3fvXt3bN68GWlpaRBFEbt27cK5c+fQv39//bwRIiIiCThYKvFwGxcMCfCAufL+5hF6a5A/7C0UOJtZgFV/XdFPgQZM0lNgOTk5qKiogKura7X1rq6uOHu29vOSmZmZtW6fmZmpW16wYAEmTJgALy8vmJmZQSaTYdmyZXjooYdq3WdpaSlKS0t1y/n5+QAAjUYDjUZT63Pqq2p/Db1fqsT+6h97rF/sr36xv/+wUgp4o38rvL3xND6LPYf+bZ3hbnvnq8zqSsoe389rSj4GSB8WLFiAgwcPYvPmzfDx8cHevXsxefJkeHh41Dh6BAAxMTGYNWtWjfU7duyAhYWFXmqMjY3Vy36pEvurf+yxfrG/+sX+VjIXAV8rOa4UVuCVlbsR2brhToVJ0ePi4uI6bytpAHJycoJcLkdWVla19VlZWXBzc6v1OW5ubnfd/vbt23j77bexYcMGDB48GAAQEBCAxMREzJ07t9YANG3aNERHR+uW8/Pz4e3tjf79+8PGxuaB3uN/aTQaxMbGol+/flAoeC+Whsb+6h97rF/sr36xvzW16FyA4YsPIPGGDNatQtCrlRNEUURxWQUKS8tRUFKOwtJyFJZWLlsq5WjmYAEPOzUUtcxYLWWPq87g1IWkAUipVCI4OBhxcXEYPnw4AECr1SIuLg5RUVG1PicsLAxxcXGYOnWqbl1sbCzCwsIA/HPaSiar/kORy+XQamtPtiqVCipVzYFjCoVCbz88fe6b2N/GwB7rF/urX+zvPwKaOeC57s2xYv9lTPoxEQq5DEWl5dCKd3+eXCbA084cPo4W8HGsvKVIMwcLeNgokV4MnEgvRHG5iPzb5cgv0SD/tgb5JeXIv61BQUk5gn3s8XzP5g36Xu7nZyr5KbDo6GhEREQgJCQEXbt2xfz581FUVITIyEgAwLhx4+Dp6YmYmBgAwJQpU9C7d298+umnGDx4MNasWYOjR49i6dKlAAAbGxv07t0bb7zxBszNzeHj44M9e/Zg9erVmDdvnmTvk4iIyFC92q8VtidlIi3vtu7O8UBlyLFSmcFabQYrVeVXfokGqbnFKNFokZpbjNTcYvx5/r97NAOOH7nrawoC8DwaNgDdD8kD0OjRo3H9+nVMnz4dmZmZ6NSpE7Zt26Yb6JyamlrtaE737t3xww8/4N1338Xbb7+NVq1aYePGjbo5gABgzZo1mDZtGsaOHYvc3Fz4+Pjggw8+wEsvvdTo74+IiMjQWasV+P2VnkjNLa4MOmozWKsUUCtktd7VXqsVkV1QipQbRUi5UYyU3CJcuVGM1BvFSM0tQrlGAycbS9haKGCjVsDG3Aw2agWs1WZ/LyvQ0sVKgnf6D8kDEABERUXd8ZTX7t27a6x74okn8MQTT9xxf25ubli5cmVDlUdERNTk2VkoYWehrNO2MpkAN1s13GzVNWad1mg02LJlCx59tKdBn2aUfCJEIiIiosbGAEREREQmhwGIiIiITA4DEBEREZkcBiAiIiIyOQxAREREZHIYgIiIiMjkMAARERGRyWEAIiIiIpPDAEREREQmhwGIiIiITA4DEBEREZkcBiAiIiIyOQxAREREZHLMpC7AEImiCADIz89v8H1rNBoUFxcjPz8fCoWiwfdv6thf/WOP9Yv91S/2V/+k7HHV7+2q3+N3wwBUi4KCAgCAt7e3xJUQERHR/SooKICtre1dtxHEusQkE6PVapGeng5ra2sIgtCg+87Pz4e3tzeuXr0KGxubBt03sb+NgT3WL/ZXv9hf/ZOyx6IooqCgAB4eHpDJ7j7Kh0eAaiGTyeDl5aXX17CxseFfPj1if/WPPdYv9le/2F/9k6rH9zryU4WDoImIiMjkMAARERGRyWEAamQqlQozZsyASqWSupQmif3VP/ZYv9hf/WJ/9c9YesxB0ERERGRyeASIiIiITA4DEBEREZkcBiAiIiIyOQxAREREZHIYgBrRokWL4OvrC7VajdDQUBw+fFjqkozW3r17MXToUHh4eEAQBGzcuLHa46IoYvr06XB3d4e5uTnCw8Nx/vx5aYo1QjExMejSpQusra3h4uKC4cOHIzk5udo2JSUlmDx5MhwdHWFlZYVRo0YhKytLooqNy+LFixEQEKCbKC4sLAxbt27VPc7eNqw5c+ZAEARMnTpVt449fjAzZ86EIAjVvvz9/XWPG0N/GYAaydq1axEdHY0ZM2YgISEBgYGBGDBgALKzs6UuzSgVFRUhMDAQixYtqvXxjz/+GF988QWWLFmCQ4cOwdLSEgMGDEBJSUkjV2qc9uzZg8mTJ+PgwYOIjY2FRqNB//79UVRUpNvm1Vdfxa+//or169djz549SE9Px8iRIyWs2nh4eXlhzpw5iI+Px9GjR/HII4/gscceQ1JSEgD2tiEdOXIEX331FQICAqqtZ48fXPv27ZGRkaH72rdvn+4xo+ivSI2ia9eu4uTJk3XLFRUVooeHhxgTEyNhVU0DAHHDhg26Za1WK7q5uYmffPKJbl1eXp6oUqnEH3/8UYIKjV92drYIQNyzZ48oipX9VCgU4vr163XbnDlzRgQgHjhwQKoyjZq9vb24fPly9rYBFRQUiK1atRJjY2PF3r17i1OmTBFFkZ/fhjBjxgwxMDCw1seMpb88AtQIysrKEB8fj/DwcN06mUyG8PBwHDhwQMLKmqbLly8jMzOzWr9tbW0RGhrKftfTrVu3AAAODg4AgPj4eGg0mmo99vf3R7Nmzdjj+1RRUYE1a9agqKgIYWFh7G0Dmjx5MgYPHlytlwA/vw3l/Pnz8PDwgJ+fH8aOHYvU1FQAxtNf3gy1EeTk5KCiogKurq7V1ru6uuLs2bMSVdV0ZWZmAkCt/a56jOpOq9Vi6tSp6NGjBzp06ACgssdKpRJ2dnbVtmWP6+7kyZMICwtDSUkJrKyssGHDBrRr1w6JiYnsbQNYs2YNEhIScOTIkRqP8fP74EJDQ7Fq1Sq0adMGGRkZmDVrFnr16oVTp04ZTX8ZgIjoriZPnoxTp05VO79PD65NmzZITEzErVu38NNPPyEiIgJ79uyRuqwm4erVq5gyZQpiY2OhVqulLqdJGjRokO77gIAAhIaGwsfHB+vWrYO5ubmEldUdT4E1AicnJ8jl8hoj4LOysuDm5iZRVU1XVU/Z7wcXFRWF3377Dbt27YKXl5duvZubG8rKypCXl1dte/a47pRKJVq2bIng4GDExMQgMDAQn3/+OXvbAOLj45GdnY3OnTvDzMwMZmZm2LNnD7744guYmZnB1dWVPW5gdnZ2aN26NS5cuGA0n2EGoEagVCoRHByMuLg43TqtVou4uDiEhYVJWFnT1Lx5c7i5uVXrd35+Pg4dOsR+15EoioiKisKGDRvwxx9/oHnz5tUeDw4OhkKhqNbj5ORkpKamssf1pNVqUVpayt42gL59++LkyZNITEzUfYWEhGDs2LG679njhlVYWIiLFy/C3d3deD7DUo/CNhVr1qwRVSqVuGrVKvH06dPihAkTRDs7OzEzM1Pq0oxSQUGBeOzYMfHYsWMiAHHevHnisWPHxJSUFFEURXHOnDminZ2duGnTJvHEiRPiY489JjZv3ly8ffu2xJUbh4kTJ4q2trbi7t27xYyMDN1XcXGxbpuXXnpJbNasmfjHH3+IR48eFcPCwsSwsDAJqzYeb731lrhnzx7x8uXL4okTJ8S33npLFARB3LFjhyiK7K0+/PsqMFFkjx/Ua6+9Ju7evVu8fPmyuH//fjE8PFx0cnISs7OzRVE0jv4yADWiBQsWiM2aNROVSqXYtWtX8eDBg1KXZLR27dolAqjxFRERIYpi5aXw7733nujq6iqqVCqxb9++YnJysrRFG5HaegtAXLlypW6b27dvi5MmTRLt7e1FCwsLccSIEWJGRoZ0RRuR559/XvTx8RGVSqXo7Ows9u3bVxd+RJG91Yf/BiD2+MGMHj1adHd3F5VKpejp6SmOHj1avHDhgu5xY+ivIIqiKM2xJyIiIiJpcAwQERERmRwGICIiIjI5DEBERERkchiAiIiIyOQwABEREZHJYQAiIiIik8MARERERCaHAYiI6A4EQcDGjRulLoOI9IABiIgM0nPPPQdBEGp8DRw4UOrSiKgJMJO6ACKiOxk4cCBWrlxZbZ1KpZKoGiJqSngEiIgMlkqlgpubW7Uve3t7AJWnpxYvXoxBgwbB3Nwcfn5++Omnn6o9/+TJk3jkkUdgbm4OR0dHTJgwAYWFhdW2WbFiBdq3bw+VSgV3d3dERUVVezwnJwcjRoyAhYUFWrVqhc2bN+seu3nzJsaOHQtnZ2eYm5ujVatWNQIbERkmBiAiMlrvvfceRo0ahePHj2Ps2LEYM2YMzpw5AwAoKirCgAEDYG9vjyNHjmD9+vXYuXNntYCzePFiTJ48GRMmTMDJkyexefNmtGzZstprzJo1C08++SROnDiBRx99FGPHjkVubq7u9U+fPo2tW7fizJkzWLx4MZycnBqvAURUf1LfjZWIqDYRERGiXC4XLS0tq3198MEHoihW3rH+pZdeqvac0NBQceLEiaIoiuLSpUtFe3t7sbCwUPf477//LspkMjEzM1MURVH08PAQ33nnnTvWAEB89913dcuFhYUiAHHr1q2iKIri0KFDxcjIyIZ5w0TUqDgGiIgM1sMPP4zFixdXW+fg4KD7PiwsrNpjYWFhSExMBACcOXMGgYGBsLS01D3eo0cPaLVaJCcnQxAEpKeno2/fvnetISAgQPe9paUlbGxskJ2dDQCYOHEiRo0ahYSEBPTv3x/Dhw9H9+7d6/VeiahxMQARkcGytLSscUqqoZibm9dpO4VCUW1ZEARotVoAwKBBg5CSkoItW7YgNjYWffv2xeTJkzF37twGr5eIGhbHABGR0Tp48GCN5bZt2wIA2rZti+PHj6OoqEj3+P79+yGTydCmTRtYW1vD19cXcXFxD1SDs7MzIiIi8N1332H+/PlYunTpA+2PiBoHjwARkcEqLS1FZmZmtXVmZma6gcbr169HSEgIevbsie+//x6HDx/G119/DQAYO3YsZsyYgYiICMycORPXr1/Hyy+/jGeffRaurq4AgJkzZ+Kll16Ci4sLBg0ahIKCAuzfvx8vv/xyneqbPn06goOD0b59e5SWluK3337TBTAiMmwMQERksLZt2wZ3d/dq69q0aYOzZ88CqLxCa82aNZg0aRLc3d3x448/ol27dgAACwsLbN++HVOmTEGXLl1gYWGBUaNGYd68ebp9RUREoKSkBJ999hlef/11ODk54fHHH69zfUqlEtOmTcOVK1dgbm6OXr16Yc2aNQ3wzolI3wRRFEWpiyAiul+CIGDDhg0YPny41KUQkRHiGCAiIiIyOQxAREREZHI4BoiIjBLP3hPRg+ARICIiIjI5DEBERERkchiAiIiIyOQwABEREZHJYQAiIiIik8MARERERCaHAYiIiIhMDgMQERERmRwGICIiIjI5/x/Wk3RakZmBWgAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(mlp.loss_curve_)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss vs. Epochs')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test MLP model\n",
    "\n",
    "We will test our model on the feature engineered test dataset. We will be using accuracy, precision, recall, F1 score, as well as a confusion matrix for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop irrelevant columns to match the training dataset\n",
    "irrelevant_columns = ['client_id', 'creation_date', 'meter_number', 'meter_code']\n",
    "X_test_prepared = X_test.drop(columns=irrelevant_columns, errors='ignore')\n",
    "\n",
    "# Standardize the test set using the same scaler used for training\n",
    "X_test_scaled = scaler_final.transform(X_test_prepared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = mlp.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Evaluation Metrics:\n",
      "Accuracy: 0.93\n",
      "Precision: 0.92\n",
      "Recall: 0.93\n",
      "F1 Score: 0.92\n",
      "\n",
      "Confusion Matrix:\n",
      "[[24753   782]\n",
      " [ 1208   356]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Not Fraud       0.95      0.97      0.96     25535\n",
      "       Fraud       0.31      0.23      0.26      1564\n",
      "\n",
      "    accuracy                           0.93     27099\n",
      "   macro avg       0.63      0.60      0.61     27099\n",
      "weighted avg       0.92      0.93      0.92     27099\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model using various metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(\"Model Evaluation Metrics:\")\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F1 Score: {f1:.2f}\")\n",
    "print(f\"\\nConfusion Matrix:\\n{conf_matrix}\")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=[\"Not Fraud\", \"Fraud\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN model\n",
    "\n",
    "We understand that one of the limitations of the MLP model is that it does not take into account each individual invoice of a client. This results in the loss of some information regarding the sequential nature of the invoices. Hence, for the RNN model, we plan to approach this with a non-aggregated dataset. We will follow the same data cleaning as the previous dataset, but without the aggregation and SMOTE. We will then use an RNN with weight class adjustments in order to deal with the imbalanced data.\n",
    "\n",
    "We will proceed with this flow:\n",
    "1. Feature engineering for RNN\n",
    "2. Preparing data for RNN\n",
    "3. Model RNN\n",
    "4. Evaluate RNN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature engineering for RNN\n",
    "\n",
    "The RNN model uses very different feature engineering techniques compared to the MLP model, and hence we need to handle them very differerntly. We will be looking at client first before invoice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Universal methods\n",
    "\n",
    "Here, we define some universal methods to make it easier for us to handle the 4 different datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to one hot encode columns in data\n",
    "def OHE(data, columns):\n",
    "    data = pd.get_dummies(data, columns=columns)\n",
    "    return data\n",
    "\n",
    "# Method to scale columns in data\n",
    "def scale(data, columns):\n",
    "    scaler = StandardScaler()\n",
    "    data[columns] = scaler.fit_transform(data[columns])\n",
    "    return data\n",
    "\n",
    "# Method to split specifically creation_date in data\n",
    "def split_date(data):\n",
    "    data['creation_date_year'] = data['creation_date'].dt.year\n",
    "    data['creation_date_month'] = data['creation_date'].dt.month\n",
    "    data['creation_date_day'] = data['creation_date'].dt.day\n",
    "    data = data.drop(columns=['creation_date'])\n",
    "    return data\n",
    "\n",
    "# Method to convert data into datetime dtype\n",
    "def datetime(data, columns):\n",
    "    data[columns] = pd.to_datetime(data[columns])\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Client\n",
    "\n",
    "Here, we will deal with the client dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### creation_date breakdown\n",
    "\n",
    "We breakdown creation_date into year, month, and day in order for our model to be able to process this feature, as it will not be able to process an object or a datetime datatype."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Creation_date to datetime and split\n",
    "client = datetime(client, 'creation_date')\n",
    "client = split_date(client)\n",
    "\n",
    "client_output = datetime(client_output, 'creation_date')\n",
    "client_output = split_date(client_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### One hot encoding\n",
    "\n",
    "We will one hot encode our non ordinal categorical features. This is because we do not want our model to assume ordinality or an order in these features, hence we have to one hot encode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List columns to one hot encode\n",
    "non_ordinal_categorical_client = ['district', 'client_catg', \n",
    "                                  'region']\n",
    "\n",
    "# Apply\n",
    "client = OHE(client, non_ordinal_categorical_client)\n",
    "client_output = OHE(client_output, non_ordinal_categorical_client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### final client\n",
    "\n",
    "We will check between the train and output datasets for any differences in their columns due to one hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in output but not in train: set()\n",
      "Columns in train but not in output: {'fraud_status', 'region_199'}\n"
     ]
    }
   ],
   "source": [
    "# Get columns as set\n",
    "df1_columns = set(client.columns)\n",
    "df2_columns = set(client_output.columns)\n",
    "\n",
    "# Check difference\n",
    "missing_in_df1 = df2_columns - df1_columns\n",
    "missing_in_df2 = df1_columns - df2_columns\n",
    "\n",
    "# Print\n",
    "if missing_in_df1 or missing_in_df2:\n",
    "    print(f\"Columns in output but not in train: {missing_in_df1}\")\n",
    "    print(f\"Columns in train but not in output: {missing_in_df2}\")\n",
    "else:\n",
    "    print(\"The DataFrames have the same columns.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Add differences\n",
    "\n",
    "As there is a difference, we will add region_199 as a boolean column with all false values. Fraud_status is not an issue as it is our label variable which the output dataset does not need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add difference\n",
    "client_output['region_199'] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Check differences\n",
    "\n",
    "Here, we will check our differences in train and output for clients again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in output but not in train: set()\n",
      "Columns in train but not in output: {'fraud_status'}\n"
     ]
    }
   ],
   "source": [
    "# Get columns as set\n",
    "df1_columns = set(client.columns)\n",
    "df2_columns = set(client_output.columns)\n",
    "\n",
    "# Check difference\n",
    "missing_in_df1 = df2_columns - df1_columns\n",
    "missing_in_df2 = df1_columns - df2_columns\n",
    "\n",
    "# Print\n",
    "if missing_in_df1 or missing_in_df2:\n",
    "    print(f\"Columns in output but not in train: {missing_in_df1}\")\n",
    "    print(f\"Columns in train but not in output: {missing_in_df2}\")\n",
    "else:\n",
    "    print(\"The DataFrames have the same columns.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Invoice\n",
    "\n",
    "Here, we will deal with the invoice dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Deal with invoice_date\n",
    "\n",
    "We will convert invoice_date to the datetime format for easier processing later down the line, as this will be the determining sequential feature that we will use for the RNN modelling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to dateTime\n",
    "invoice = datetime(invoice, 'invoice_date')\n",
    "\n",
    "invoice_output = datetime(invoice_output, 'invoice_date')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Scaling \n",
    "\n",
    "We will scale continuous variables, to ensure that all features contribute equally when it comes to the gradient descent during optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set columns to scale\n",
    "continuous_features_invoice = ['consumption_level_1', 'consumption_level_2',\n",
    "                       'consumption_level_3', 'consumption_level_4',\n",
    "                        'diff_in_index', 'old_index', 'months_number']\n",
    "\n",
    "# Apply\n",
    "invoice = scale(invoice, continuous_features_invoice)\n",
    "invoice_output = scale(invoice_output, continuous_features_invoice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### One hot encoding\n",
    "\n",
    "We will similarly one hot encode the non ordinal categorical features in invoice for the same reasons as we did for client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set columns to one hot encode\n",
    "non_ordinal_categorical_invoice = [ 'meter_status', 'meter_code', \n",
    "                            'reading_remark', 'meter_type',\n",
    "                            'meter_coefficient']\n",
    "\n",
    "# Apply\n",
    "invoice = OHE(invoice, non_ordinal_categorical_invoice)\n",
    "invoice_output = OHE(invoice_output, non_ordinal_categorical_invoice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### final invoice\n",
    "\n",
    "We will check between the train and output datasets for any differences in their columns due to one hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in output but not in train: set()\n",
      "Columns in train but not in output: {'meter_coefficient_11', 'meter_coefficient_33', 'meter_coefficient_40', 'meter_coefficient_20', 'meter_coefficient_50', 'meter_code_367', 'meter_coefficient_30'}\n"
     ]
    }
   ],
   "source": [
    "# Get columns as set\n",
    "df1_columns = set(invoice.columns)\n",
    "df2_columns = set(invoice_output.columns)\n",
    "\n",
    "# Check difference\n",
    "missing_in_df1 = df2_columns - df1_columns\n",
    "missing_in_df2 = df1_columns - df2_columns\n",
    "\n",
    "# Print\n",
    "if missing_in_df1 or missing_in_df2:\n",
    "    print(f\"Columns in output but not in train: {missing_in_df1}\")\n",
    "    print(f\"Columns in train but not in output: {missing_in_df2}\")\n",
    "else:\n",
    "    print(\"The DataFrames have the same columns.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Add differences\n",
    "\n",
    "As all the different columns are the one hot encoded variables, we will iteratively add them as boolean columns that are False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add difference\n",
    "for column in missing_in_df2:\n",
    "    invoice_output[column] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Check differences\n",
    "\n",
    "Here, we will check our differences in train and output for invoice again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The DataFrames have the same columns.\n"
     ]
    }
   ],
   "source": [
    "# Get columns as set\n",
    "df1_columns = set(invoice.columns)\n",
    "df2_columns = set(invoice_output.columns)\n",
    "\n",
    "# Check difference\n",
    "missing_in_df1 = df2_columns - df1_columns\n",
    "missing_in_df2 = df1_columns - df2_columns\n",
    "\n",
    "# Print\n",
    "if missing_in_df1 or missing_in_df2:\n",
    "    print(f\"Columns in output but not in train: {missing_in_df1}\")\n",
    "    print(f\"Columns in train but not in output: {missing_in_df2}\")\n",
    "else:\n",
    "    print(\"The DataFrames have the same columns.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing data for RNN\n",
    "\n",
    "In order to prepare our dataset for RNN, we will need to firstly group our invoices by clients, in order of invoice date. We will then be able to extract their informational features. We also need to extract, for the client, their client data, as well as label data (fraud or not fraud). This is so that our RNN is able to use the sequential nature of invoices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get informational columns\n",
    "\n",
    "We will obtain the informational features of both invoice and clients, which as the features that we want our model to learn from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invoice feature columns\n",
    "invoice_feature_cols = invoice.columns.difference(['client_id', 'invoice_date'])\n",
    "\n",
    "# Client feature columns\n",
    "client_feature_cols = client.columns.difference(['client_id', 'fraud_status'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sort the dataset\n",
    "\n",
    "We will sort our dataset by client_id and invoice_date, followed by grouping invoices by client_id. This helps us obtain the sequences of of invoices that the RNN will input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort and group \n",
    "invoice = invoice.sort_values(by=['client_id', 'invoice_date'])\n",
    "grouped_invoices = invoice.groupby('client_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort and group\n",
    "invoice_output = invoice_output.sort_values(by=['client_id', 'invoice_date'])\n",
    "grouped_invoices_output = invoice_output.groupby('client_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Obtain max_length\n",
    "\n",
    "We need to obtain the max_length of each group (max number of invoices per client), as only then are we able to apply padding to our sequences to ensure that they are all of the same length. We will use the max between both train and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get max length\n",
    "max_length = grouped_invoices.size().max()\n",
    "max_length1 = grouped_invoices_output.size().max()\n",
    "\n",
    "# Get max length among both\n",
    "if max_length < max_length1:\n",
    "    max_length = max_length1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create containers\n",
    "\n",
    "We will create 3 containers to store the features and targets, and 2 containers to prepare the test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "invoice_features = []\n",
    "client_features = []\n",
    "targets = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### iterate each client\n",
    "\n",
    "For each client, we will select informational invoice features, pad them, and add them to invoice features, then obtain their informational client features and add them to client features, followed by adding their target to the targets container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate for each client_id\n",
    "for client_id, group in grouped_invoices:\n",
    "    # Get meaningful features\n",
    "    invoices_seq = group[invoice_feature_cols].values\n",
    "\n",
    "    # Apply padding\n",
    "    padded_invoices = np.zeros((max_length, invoices_seq.shape[1]))\n",
    "    padded_invoices[:len(invoices_seq)] = invoices_seq\n",
    "\n",
    "    # Add to container\n",
    "    invoice_features.append(padded_invoices)\n",
    "    \n",
    "    # Get meaningful features\n",
    "    client_row = client[client['client_id'] == client_id].iloc[0]\n",
    "    client_feat = client_row[client_feature_cols].values\n",
    "    client_feat = client_feat.astype(int)\n",
    "\n",
    "    # Add to container\n",
    "    client_features.append(client_feat)\n",
    "    \n",
    "    # Append target\n",
    "    targets.append(client_row['fraud_status'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pytorch dataset\n",
    "\n",
    "We will create a custom pytorch dataset to wrap the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FraudDataset(Dataset):\n",
    "    def __init__(self, invoice_features, client_features, targets):\n",
    "\n",
    "        # Invoice features\n",
    "        self.invoice_features = torch.tensor(invoice_features, dtype=torch.float32)\n",
    "\n",
    "        # Client features\n",
    "        self.client_features = torch.tensor(client_features, dtype=torch.float32)\n",
    "\n",
    "        # Targets\n",
    "        self.targets = torch.tensor(targets, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.targets)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'invoices': self.invoice_features[idx],\n",
    "            'clients': self.client_features[idx],\n",
    "            'target': self.targets[idx]\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert to np.array\n",
    "\n",
    "We will convert our train and test datasets to np.array for easier use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to np.arr\n",
    "client_features = np.array(client_features, dtype=np.float32)\n",
    "invoice_features = np.array(invoice_features, dtype=np.float32)\n",
    "targets = np.array(targets, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Test Split\n",
    "\n",
    "We will split the train data into train/test. This is because our test dataset does not have a target, hence cannot be used to test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test split\n",
    "invoice_train, invoice_test, client_train, client_test, target_train, target_test = train_test_split(\n",
    "    invoice_features, client_features, targets, test_size=0.2, random_state=42, stratify=targets\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create class weights\n",
    "\n",
    "Due to an imbalance in the classes, we will create class weights based on the proportion of the class distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights shape: torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "# Set classes\n",
    "classes = np.array([0, 1])\n",
    "\n",
    "# Compute class weights\n",
    "class_weights = compute_class_weight('balanced', classes=classes, y=target_train)\n",
    "\n",
    "# Convert to tensor\n",
    "weights = torch.tensor(class_weights, dtype=torch.float32) \n",
    "\n",
    "# Check the shape of weights\n",
    "print(f\"Class weights shape: {weights.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weighted random sampler\n",
    "\n",
    "We will create a weighted random sampler, that will use class weights as a sampling probability. This thus ensures that in each batch that is fed into the model, there is an equal distribution of both classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change train to int\n",
    "target_train = target_train.astype(int)\n",
    "\n",
    "# Assign sample weights based on the target labels\n",
    "train_weights = torch.tensor([weights[t] for t in target_train], dtype=torch.float32)\n",
    "\n",
    "# Create WeightedRandomSampler\n",
    "sampler = WeightedRandomSampler(weights=train_weights, num_samples=len(target_train), replacement=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input into dataloader\n",
    "\n",
    "We will load our data into a dataloader to feed into our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input into dataloader\n",
    "train_dataset = FraudDataset(invoice_train, client_train, target_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, sampler=sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input into dataloader\n",
    "test_dataset = FraudDataset(invoice_test, client_test, target_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create our model\n",
    "\n",
    "We will create the method of our model. This will be the main method that we will use to run all our code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create FraudRNN class\n",
    "\n",
    "We will create the FraudRNN class. To initialize the model, we need to input the parameters listed below:\n",
    "\n",
    "1. seq_input_dim\n",
    "2. client_input_dim\n",
    "3. hidden_dim\n",
    "4. output_dim\n",
    "5. num_layers\n",
    "6. dropout\n",
    "\n",
    "It will use num_layers number of LSTM layers, as well as a single fully connected layer, and a single output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create fraud RNN class\n",
    "class FraudRNN(nn.Module):\n",
    "    def __init__(self, seq_input_dim, client_input_dim, hidden_dim, output_dim, num_layers=1, dropout=0.5):\n",
    "        super(FraudRNN, self).__init__()\n",
    "\n",
    "        # LSTM layers\n",
    "        self.lstm = nn.LSTM(input_size=seq_input_dim, hidden_size=hidden_dim, num_layers=num_layers, batch_first=True, dropout=dropout, bidirectional=True)\n",
    "\n",
    "        # FCL\n",
    "        self.client_dense = nn.Linear(client_input_dim, hidden_dim)\n",
    "\n",
    "        # Output layer\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "    \n",
    "    def forward(self, x_seq, x_client):\n",
    "        _, (h_n, _) = self.lstm(x_seq)\n",
    "        lstm_out = h_n[-1]\n",
    "        client_out = torch.relu(self.client_dense(x_client))\n",
    "        combined = torch.cat((lstm_out, client_out), dim=1)\n",
    "        output = self.fc(combined)  # Output a single logit per sample\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create xavier initialization\n",
    "\n",
    "We will also use the xavier initialization in order to have better weights initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Xavier initialization function\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "\n",
    "        # Apply Xavier initialization to Linear layers\n",
    "        init.xavier_uniform_(m.weight) \n",
    "        if m.bias is not None:\n",
    "\n",
    "            # Initialize bias to zero\n",
    "            init.zeros_(m.bias) \n",
    "    elif isinstance(m, nn.LSTM):\n",
    "        for name, param in m.named_parameters():\n",
    "\n",
    "            # Input-to-hidden weights\n",
    "            if 'weight_ih' in name: \n",
    "                init.xavier_uniform_(param)\n",
    "            \n",
    "            # Hidden-to-hidden weights\n",
    "            elif 'weight_hh' in name:\n",
    "                init.orthogonal_(param)\n",
    "            \n",
    "            # Orthonormal initialization for recurrent weights\n",
    "            elif 'bias' in name:\n",
    "                init.zeros_(param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning\n",
    "\n",
    "We will perform hyperparameter tuning over a subset of hyperparameter values. This will be performed on a 10% sample of our data, as RNN is computationally expensive and our dataset is large. Additionally, we will not be using k-folds cross validation when conducting hyperparameter tuning, again due to computational time. Ideally, given proper set up, we would like to use k-folds cross validation to conduct a gridsearch across a large selection of hyperparameters on the full dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating sampled data\n",
    "\n",
    "Here, we will first set our sample size. We then randomly select indices from the length of our full dataset without replacement. We then proceed to select those indices from our invoice, client and target train. We then assign create a train_weights_sampled, which will be the weights of each item in our sampled dataset. We then create a weighted random sampler for our sampled dataset, and load it with a dataloader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10% of the total dataset size\n",
    "sample_size = int(0.1 * len(train_dataset)) \n",
    "\n",
    "# Random sample\n",
    "indices = np.random.choice(len(invoice_train), size=sample_size, replace=False)\n",
    "\n",
    "# Sample all\n",
    "invoice_sampled = invoice_train[indices]\n",
    "client_sampled = client_train[indices]\n",
    "targets_sampled = target_train[indices]\n",
    "\n",
    "# Assign sample weights based on the target labels\n",
    "train_weights_sampled = torch.tensor([weights[t] for t in targets_sampled], dtype=torch.float32)\n",
    "\n",
    "# Create WeightedRandomSampler\n",
    "sampler_sampled = WeightedRandomSampler(weights=train_weights_sampled, num_samples=sample_size, replacement=True)\n",
    "\n",
    "# Load to dataloader\n",
    "sampled_dataset = FraudDataset(invoice_sampled, client_sampled, targets_sampled)\n",
    "sampled_loader = DataLoader(sampled_dataset, batch_size=16, sampler=sampler_sampled)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set fixed parameters\n",
    "\n",
    "Here we will set seq_input_dim_sampled and client_input_dim_sampled to be fixed values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample parameters\n",
    "seq_input_dim_sampled = np.array(invoice_sampled).shape[2]\n",
    "client_input_dim_sampled = np.array(client_sampled).shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GridSearch across hyperparameter values\n",
    "\n",
    "We will perform a gridsearch across all combinations of a hyperparameter values to obtain the best parameters and best model results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with params: {'dropout': 0.3, 'hidden_dim': 32, 'learning_rate': 0.001, 'num_layers': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.0559, Precision: 0.0559, Recall: 1.0000, F1 Score: 0.1058\n",
      "Training with params: {'dropout': 0.3, 'hidden_dim': 32, 'learning_rate': 0.001, 'num_layers': 2}\n",
      "Accuracy: 0.3726, Precision: 0.0753, Recall: 0.9075, F1 Score: 0.1391\n",
      "Training with params: {'dropout': 0.3, 'hidden_dim': 32, 'learning_rate': 0.001, 'num_layers': 3}\n",
      "Accuracy: 0.5032, Precision: 0.0834, Recall: 0.7905, F1 Score: 0.1509\n",
      "Training with params: {'dropout': 0.3, 'hidden_dim': 32, 'learning_rate': 0.01, 'num_layers': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.0590, Precision: 0.0558, Recall: 0.9947, F1 Score: 0.1056\n",
      "Training with params: {'dropout': 0.3, 'hidden_dim': 32, 'learning_rate': 0.01, 'num_layers': 2}\n",
      "Accuracy: 0.1294, Precision: 0.0599, Recall: 0.9927, F1 Score: 0.1130\n",
      "Training with params: {'dropout': 0.3, 'hidden_dim': 32, 'learning_rate': 0.01, 'num_layers': 3}\n",
      "Accuracy: 0.4270, Precision: 0.0784, Recall: 0.8612, F1 Score: 0.1438\n",
      "Training with params: {'dropout': 0.3, 'hidden_dim': 64, 'learning_rate': 0.001, 'num_layers': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.3367, Precision: 0.0730, Recall: 0.9299, F1 Score: 0.1354\n",
      "Training with params: {'dropout': 0.3, 'hidden_dim': 64, 'learning_rate': 0.001, 'num_layers': 2}\n",
      "Accuracy: 0.7478, Precision: 0.1036, Recall: 0.4594, F1 Score: 0.1691\n",
      "Training with params: {'dropout': 0.3, 'hidden_dim': 64, 'learning_rate': 0.001, 'num_layers': 3}\n",
      "Accuracy: 0.9088, Precision: 0.1493, Recall: 0.1348, F1 Score: 0.1417\n",
      "Training with params: {'dropout': 0.3, 'hidden_dim': 64, 'learning_rate': 0.01, 'num_layers': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9237, Precision: 0.0962, Recall: 0.0436, F1 Score: 0.0600\n",
      "Training with params: {'dropout': 0.3, 'hidden_dim': 64, 'learning_rate': 0.01, 'num_layers': 2}\n",
      "Accuracy: 0.1293, Precision: 0.0599, Recall: 0.9927, F1 Score: 0.1130\n",
      "Training with params: {'dropout': 0.3, 'hidden_dim': 64, 'learning_rate': 0.01, 'num_layers': 3}\n",
      "Accuracy: 0.9441, Precision: 1.0000, Recall: 0.0000, F1 Score: 0.0000\n",
      "Training with params: {'dropout': 0.3, 'hidden_dim': 128, 'learning_rate': 0.001, 'num_layers': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.0638, Precision: 0.0562, Recall: 0.9987, F1 Score: 0.1065\n",
      "Training with params: {'dropout': 0.3, 'hidden_dim': 128, 'learning_rate': 0.001, 'num_layers': 2}\n",
      "Accuracy: 0.7100, Precision: 0.1016, Recall: 0.5347, F1 Score: 0.1708\n",
      "Training with params: {'dropout': 0.3, 'hidden_dim': 128, 'learning_rate': 0.001, 'num_layers': 3}\n",
      "Accuracy: 0.9441, Precision: 1.0000, Recall: 0.0000, F1 Score: 0.0000\n",
      "Training with params: {'dropout': 0.3, 'hidden_dim': 128, 'learning_rate': 0.01, 'num_layers': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9437, Precision: 0.3103, Recall: 0.0059, F1 Score: 0.0117\n",
      "Training with params: {'dropout': 0.3, 'hidden_dim': 128, 'learning_rate': 0.01, 'num_layers': 2}\n",
      "Accuracy: 0.0913, Precision: 0.0576, Recall: 0.9941, F1 Score: 0.1089\n",
      "Training with params: {'dropout': 0.3, 'hidden_dim': 128, 'learning_rate': 0.01, 'num_layers': 3}\n",
      "Accuracy: 0.0559, Precision: 0.0559, Recall: 1.0000, F1 Score: 0.1058\n",
      "Training with params: {'dropout': 0.5, 'hidden_dim': 32, 'learning_rate': 0.001, 'num_layers': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6929, Precision: 0.0931, Recall: 0.5142, F1 Score: 0.1576\n",
      "Training with params: {'dropout': 0.5, 'hidden_dim': 32, 'learning_rate': 0.001, 'num_layers': 2}\n",
      "Accuracy: 0.7116, Precision: 0.1052, Recall: 0.5545, F1 Score: 0.1768\n",
      "Training with params: {'dropout': 0.5, 'hidden_dim': 32, 'learning_rate': 0.001, 'num_layers': 3}\n",
      "Accuracy: 0.1865, Precision: 0.0634, Recall: 0.9855, F1 Score: 0.1192\n",
      "Training with params: {'dropout': 0.5, 'hidden_dim': 32, 'learning_rate': 0.01, 'num_layers': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.0589, Precision: 0.0559, Recall: 0.9980, F1 Score: 0.1059\n",
      "Training with params: {'dropout': 0.5, 'hidden_dim': 32, 'learning_rate': 0.01, 'num_layers': 2}\n",
      "Accuracy: 0.4026, Precision: 0.0766, Recall: 0.8764, F1 Score: 0.1408\n",
      "Training with params: {'dropout': 0.5, 'hidden_dim': 32, 'learning_rate': 0.01, 'num_layers': 3}\n",
      "Accuracy: 0.9438, Precision: 0.2222, Recall: 0.0026, F1 Score: 0.0052\n",
      "Training with params: {'dropout': 0.5, 'hidden_dim': 64, 'learning_rate': 0.001, 'num_layers': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9416, Precision: 0.2083, Recall: 0.0165, F1 Score: 0.0306\n",
      "Training with params: {'dropout': 0.5, 'hidden_dim': 64, 'learning_rate': 0.001, 'num_layers': 2}\n",
      "Accuracy: 0.7234, Precision: 0.1059, Recall: 0.5307, F1 Score: 0.1765\n",
      "Training with params: {'dropout': 0.5, 'hidden_dim': 64, 'learning_rate': 0.001, 'num_layers': 3}\n",
      "Accuracy: 0.6978, Precision: 0.1025, Recall: 0.5684, F1 Score: 0.1736\n",
      "Training with params: {'dropout': 0.5, 'hidden_dim': 64, 'learning_rate': 0.01, 'num_layers': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.0889, Precision: 0.0575, Recall: 0.9954, F1 Score: 0.1088\n",
      "Training with params: {'dropout': 0.5, 'hidden_dim': 64, 'learning_rate': 0.01, 'num_layers': 2}\n",
      "Accuracy: 0.9359, Precision: 0.1056, Recall: 0.0198, F1 Score: 0.0334\n",
      "Training with params: {'dropout': 0.5, 'hidden_dim': 64, 'learning_rate': 0.01, 'num_layers': 3}\n",
      "Accuracy: 0.0559, Precision: 0.0559, Recall: 1.0000, F1 Score: 0.1058\n",
      "Training with params: {'dropout': 0.5, 'hidden_dim': 128, 'learning_rate': 0.001, 'num_layers': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.4517, Precision: 0.0691, Recall: 0.7072, F1 Score: 0.1259\n",
      "Training with params: {'dropout': 0.5, 'hidden_dim': 128, 'learning_rate': 0.001, 'num_layers': 2}\n",
      "Accuracy: 0.3257, Precision: 0.0719, Recall: 0.9299, F1 Score: 0.1335\n",
      "Training with params: {'dropout': 0.5, 'hidden_dim': 128, 'learning_rate': 0.001, 'num_layers': 3}\n",
      "Accuracy: 0.9416, Precision: 0.2587, Recall: 0.0245, F1 Score: 0.0447\n",
      "Training with params: {'dropout': 0.5, 'hidden_dim': 128, 'learning_rate': 0.01, 'num_layers': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9431, Precision: 0.2157, Recall: 0.0073, F1 Score: 0.0141\n",
      "Training with params: {'dropout': 0.5, 'hidden_dim': 128, 'learning_rate': 0.01, 'num_layers': 2}\n",
      "Accuracy: 0.9439, Precision: 0.1111, Recall: 0.0007, F1 Score: 0.0013\n",
      "Training with params: {'dropout': 0.5, 'hidden_dim': 128, 'learning_rate': 0.01, 'num_layers': 3}\n",
      "Accuracy: 0.0559, Precision: 0.0559, Recall: 1.0000, F1 Score: 0.1058\n",
      "Best parameters: {'dropout': 0.5, 'hidden_dim': 32, 'learning_rate': 0.001, 'num_layers': 2}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Train and evaluate model\n",
    "def train_and_evaluate_model(model, train_loader, test_loader, optimizer, criterion, num_epochs=10):\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        # Train model\n",
    "        model.train()\n",
    "\n",
    "        # Set loss\n",
    "        total_loss = 0\n",
    "\n",
    "        # Iterate for each batch\n",
    "        for batch in train_loader:\n",
    "\n",
    "            # Retrieve info\n",
    "            X_seq_batch = batch['invoices']\n",
    "            X_client_batch = batch['clients']\n",
    "            y_batch = batch['target']\n",
    "\n",
    "            # Reset optimizer\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Get the model's raw logits\n",
    "            y_pred = model(X_seq_batch, X_client_batch).squeeze()\n",
    "\n",
    "            # Ensure y_batch is 1D (flatten it)\n",
    "            y_batch = y_batch.squeeze()\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = criterion(y_pred, y_batch)\n",
    "\n",
    "            # Backpropogation\n",
    "            loss.backward()\n",
    "\n",
    "            # Optimizer\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Count loss\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    # Evaluate\n",
    "    model.eval()\n",
    "\n",
    "    # Set target and predictions\n",
    "    all_targets = []\n",
    "    all_predictions = []\n",
    "\n",
    "    # Evaluate\n",
    "    with torch.no_grad():\n",
    "\n",
    "        # Iterate for each batch\n",
    "        for batch in test_loader:\n",
    "\n",
    "            # Obtain info\n",
    "            X_seq_batch = batch['invoices']\n",
    "            X_client_batch = batch['clients']\n",
    "            y_batch = batch['target']\n",
    "\n",
    "            # Get raw logits\n",
    "            y_pred_logits = model(X_seq_batch, X_client_batch).squeeze()\n",
    "\n",
    "            # Apply sigmoid to get probabilities\n",
    "            y_pred_probs = torch.sigmoid(y_pred_logits)\n",
    "\n",
    "            # Convert probabilities to binary predictions\n",
    "            y_pred_binary = (y_pred_probs > 0.5).float()\n",
    "\n",
    "            # Store predictions and true values\n",
    "            all_predictions.extend(y_pred_binary.cpu().numpy())\n",
    "            all_targets.extend(y_batch.cpu().numpy())\n",
    "\n",
    "    # Calculate evaluation metrics\n",
    "    accuracy = accuracy_score(all_targets, all_predictions)\n",
    "    precision = precision_score(all_targets, all_predictions, zero_division=1)\n",
    "    recall = recall_score(all_targets, all_predictions, zero_division=1)\n",
    "    f1 = f1_score(all_targets, all_predictions, zero_division=1)\n",
    "\n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "# Define the hyperparameter grid to search over\n",
    "param_grid = {\n",
    "\n",
    "    # Example values for hidden dimension\n",
    "    'hidden_dim': [32, 64, 128],\n",
    "\n",
    "    # Example values for LSTM layers\n",
    "    'num_layers': [1, 2, 3],\n",
    "\n",
    "    # Example learning rates\n",
    "    'learning_rate': [0.001, 0.01], \n",
    "\n",
    "    # Example dropout values \n",
    "    'dropout': [0.3, 0.5]  \n",
    "}\n",
    "\n",
    "# Get all combinations of hyperparameters\n",
    "grid = ParameterGrid(param_grid)\n",
    "\n",
    "best_score = 0\n",
    "best_params = {}\n",
    "\n",
    "# Grid Search Loop\n",
    "for params in grid:\n",
    "    print(f\"Training with params: {params}\")\n",
    "    \n",
    "    # Initialize model with current hyperparameters\n",
    "    model = FraudRNN(seq_input_dim_sampled, client_input_dim_sampled, hidden_dim=params['hidden_dim'], output_dim=1, \n",
    "                     num_layers=params['num_layers'], dropout=params['dropout'])\n",
    "    \n",
    "    # Initialize optimizer and loss function\n",
    "    optimizer = optim.Adam(model.parameters(), lr=params['learning_rate'])\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    # Train and evaluate the model\n",
    "    accuracy, precision, recall, f1 = train_and_evaluate_model(model, sampled_loader, test_loader, optimizer, criterion)\n",
    "\n",
    "    print(f\"Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}\")\n",
    "    \n",
    "    # Save the best performing model\n",
    "    if f1 > best_score:\n",
    "        best_score = f1\n",
    "        best_params = params\n",
    "\n",
    "print(f\"Best parameters: {best_params}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and test RNN model\n",
    "\n",
    "Here, we will start training our best model with the best determined hyperparameters on the full dataset, and obtain evaluation metrics, with and without xavier initialization to compare."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model without xavier initialization\n",
    "\n",
    "We will train and test the model with the full dataset without xavier initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set fixed parameters\n",
    "seq_input_dim = np.array(invoice_train).shape[2]\n",
    "client_input_dim = np.array(client_train).shape[1]\n",
    "\n",
    "# Initialize model\n",
    "model = FraudRNN(seq_input_dim, client_input_dim, hidden_dim=best_params['hidden_dim'], output_dim=1, \n",
    "                 num_layers=best_params['num_layers'], dropout=best_params['dropout'])\n",
    "    \n",
    "# Initialize optimizer and loss function\n",
    "optimizer = optim.Adam(model.parameters(), lr=best_params['learning_rate'])\n",
    "criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5314, Precision: 0.0825, Recall: 0.7303, F1 Score: 0.1483\n"
     ]
    }
   ],
   "source": [
    "# Train and get evaluation metrics\n",
    "accuracy, precision, recall, f1 = train_and_evaluate_model(model, train_loader, test_loader, optimizer, criterion)\n",
    "\n",
    "# Print\n",
    "print(f\"Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model with xavier initialization\n",
    "\n",
    "We will train and test the model with the full dataset with xavier initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "model = FraudRNN(seq_input_dim, client_input_dim, hidden_dim=best_params['hidden_dim'], output_dim=1, \n",
    "                 num_layers=best_params['num_layers'], dropout=best_params['dropout'])\n",
    "    \n",
    "# Initialize optimizer, xavier initalization and loss function\n",
    "model.apply(init_weights)\n",
    "optimizer = optim.Adam(model.parameters(), lr=best_params['learning_rate'])\n",
    "criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6355, Precision: 0.0971, Recall: 0.6662, F1 Score: 0.1696\n"
     ]
    }
   ],
   "source": [
    "# Train and get evaluation metrics\n",
    "accuracy, precision, recall, f1 = train_and_evaluate_model(model, train_loader, test_loader, optimizer, criterion)\n",
    "\n",
    "# Print\n",
    "print(f\"Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model wtih xavier initalization is able to perform better than without!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full evaluation of best RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "model.eval()\n",
    "\n",
    "# Set target and predictions\n",
    "all_targets = []\n",
    "all_predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    # iterate for each batch\n",
    "    for batch in test_loader:\n",
    "\n",
    "        # Get info\n",
    "        X_seq_batch = batch['invoices']\n",
    "        X_client_batch = batch['clients']\n",
    "        y_batch = batch['target']\n",
    "\n",
    "        # Get raw logits\n",
    "        y_pred_logits = model(X_seq_batch, X_client_batch).squeeze()\n",
    "\n",
    "        # Apply sigmoid to get probabilities\n",
    "        y_pred_probs = torch.sigmoid(y_pred_logits)\n",
    "\n",
    "        # Convert probabilities to binary predictions\n",
    "        y_pred_binary = (y_pred_probs > 0.5).float()\n",
    "\n",
    "        # Store predictions and true values\n",
    "        all_predictions.extend(y_pred_binary.cpu().numpy())\n",
    "        all_targets.extend(y_batch.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.97      0.63      0.77     25572\n",
      "         1.0       0.10      0.67      0.17      1513\n",
      "\n",
      "    accuracy                           0.64     27085\n",
      "   macro avg       0.53      0.65      0.47     27085\n",
      "weighted avg       0.92      0.64      0.73     27085\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print full class report\n",
    "class_report = classification_report(all_targets, all_predictions)\n",
    "print(class_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, the evaluation for the RNN model depends on how we weigh misclassifications. \n",
    "\n",
    "Looking at the classification report, we see a between 63% to 67%, which indicates that we have around 35% misclassifications for both classes. However, when looking at the precision of fraud, we can see that 90% of predicted positive cases were in fact not fraud. \n",
    "\n",
    "This issue with low precision comes from the imbalanced test set. Since we have a large proportion of data from the test set being non-fraud, it potentially polutes the evaluation results.\n",
    "\n",
    "Overall, we can see that the RNN model performs decently. We will need to compare it to other models to see how it fairs, before determining if it is the best model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
